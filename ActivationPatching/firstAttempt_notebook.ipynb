{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17e8fe4c32f734fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(model_name: str = 'gpt2'):\n",
    "    \"\"\"\n",
    "    Loads a pretrained GPT-2 model and its tokenizer.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model and tokenizer for '{model_name}'...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74224b1c7fca6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Defines the clean and corrupted prompts and answers for each task.\n",
    "    \"\"\"\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs No Rule)\",\n",
    "            \"clean_prompt\": \"ABCD:ABCE::JKLM:JKL\",\n",
    "            \"clean_correct_answer\": \"N\",\n",
    "            \"clean_incorrect_answer\": \"M\",\n",
    "            \"corrupted_prompt\": \"ABCD:ABCD::JKLM:JKL\",\n",
    "            \"corrupted_correct_answer\": \"M\",\n",
    "            \"corrupted_incorrect_answer\": \"N\",\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"clean_prompt\": \"January:February::Wednesday:\",\n",
    "            \"clean_correct_answer\": \"Thursday\",\n",
    "            \"clean_incorrect_answer\": \"Wednesday\",\n",
    "            \"corrupted_prompt\": \"January:January::Wednesday:\",\n",
    "            \"corrupted_correct_answer\": \"Wednesday\",\n",
    "            \"corrupted_incorrect_answer\": \"Thursday\",\n",
    "        }\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45e58773797e47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "def caching_hook_factory(hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        activation_cache[hook_name] = tensor_to_cache.detach()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    \"\"\"Factory for patching hooks. FIX: Handles mismatched sequence lengths.\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in activation_cache:\n",
    "            raise ValueError(f\"Activation for {hook_name} not found in cache!\")\n",
    "        cached_activation = activation_cache[hook_name]\n",
    "\n",
    "        patched_output = output.clone()\n",
    "        \n",
    "        # Determine the minimum sequence length to avoid shape mismatch errors.\n",
    "        # The sequence length is the second-to-last dimension.\n",
    "        min_seq_len = min(patched_output.shape[-2], cached_activation.shape[-2])\n",
    "\n",
    "        if head_index is not None: # Attention head patch\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            if patched_output.ndim == 3:\n",
    "                # Patch for [batch, seq_len, embed_dim]\n",
    "                patched_output[:, :min_seq_len, start:end] = cached_activation[:, :min_seq_len, start:end]\n",
    "            elif patched_output.ndim == 2:\n",
    "                # Patch for [seq_len, embed_dim]\n",
    "                patched_output[:min_seq_len, start:end] = cached_activation[:min_seq_len, start:end]\n",
    "        else: # MLP patch\n",
    "            if patched_output.ndim == 3:\n",
    "                # Patch for [batch, seq_len, embed_dim]\n",
    "                patched_output[:, :min_seq_len, :] = cached_activation[:, :min_seq_len, :]\n",
    "            elif patched_output.ndim == 2:\n",
    "                # Patch for [seq_len, embed_dim]\n",
    "                patched_output[:min_seq_len, :] = cached_activation[:min_seq_len, :]\n",
    "        \n",
    "        return patched_output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3479e78a1b69d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'):\n",
    "        model = getattr(model, part)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4df82e128c02df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_hooks(\n",
    "        model: nn.Module,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        prompt: str,\n",
    "        hook_fns: Dict[str, Callable],\n",
    ") -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module_name = name.split('_')[0] if 'attn.c_proj' in name else name\n",
    "            module = get_module_by_name(model, name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7b673729edf14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logit_diff(\n",
    "        logits: torch.Tensor,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        correct_answer: str,\n",
    "        incorrect_answer: str\n",
    ") -> float:\n",
    "    correct_id = tokenizer.encode(correct_answer, add_prefix_space=False)[0]\n",
    "    incorrect_id = tokenizer.encode(incorrect_answer, add_prefix_space=False)[0]\n",
    "    return (logits[correct_id] - logits[incorrect_id]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10818139b27878ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_patching_experiment(\n",
    "        model: nn.Module,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        source_prompt: str,\n",
    "        dest_prompt: str,\n",
    "        dest_correct_answer: str,\n",
    "        dest_incorrect_answer: str,\n",
    "        layer: int,\n",
    "        component_type: str,\n",
    "        head_index: int = None\n",
    ") -> float:\n",
    "    global activation_cache\n",
    "    activation_cache = {}\n",
    "\n",
    "    if component_type == 'attn_head':\n",
    "        hook_name = f\"transformer.h.{layer}.attn.c_proj\"\n",
    "    elif component_type == 'mlp':\n",
    "        hook_name = f\"transformer.h.{layer}.mlp.c_proj\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid component type\")\n",
    "\n",
    "    caching_hooks = {hook_name: caching_hook_factory(hook_name)}\n",
    "    run_with_hooks(model, tokenizer, source_prompt, caching_hooks)\n",
    "\n",
    "    d_head = model.config.n_embd // model.config.n_head if component_type == 'attn_head' else None\n",
    "    patching_hooks = {hook_name: patching_hook_factory(hook_name, head_index, d_head)}\n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(\n",
    "        model: nn.Module,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        task_data: Dict,\n",
    "        patch_type: str\n",
    ") -> pd.DataFrame:\n",
    "    n_layers = model.config.n_layer\n",
    "    n_heads = model.config.n_head\n",
    "\n",
    "    if patch_type == 'noising':\n",
    "        source_prompt = task_data['corrupted_prompt']\n",
    "        dest_prompt = task_data['clean_prompt']\n",
    "        dest_correct_answer = task_data['clean_correct_answer']\n",
    "        dest_incorrect_answer = task_data['clean_incorrect_answer']\n",
    "    elif patch_type == 'denoising':\n",
    "        source_prompt = task_data['clean_prompt']\n",
    "        dest_prompt = task_data['corrupted_prompt']\n",
    "        dest_correct_answer = task_data['corrupted_correct_answer']\n",
    "        dest_incorrect_answer = task_data['corrupted_incorrect_answer']\n",
    "    else:\n",
    "        raise ValueError(\"patch_type must be 'noising' or 'denoising'\")\n",
    "\n",
    "    clean_logits = run_with_hooks(model, tokenizer, task_data['clean_prompt'], {})\n",
    "    clean_baseline = calculate_logit_diff(clean_logits, tokenizer, task_data['clean_correct_answer'], task_data['clean_incorrect_answer'])\n",
    "\n",
    "    corrupted_logits = run_with_hooks(model, tokenizer, task_data['corrupted_prompt'], {})\n",
    "    corrupted_baseline = calculate_logit_diff(corrupted_logits, tokenizer, task_data['corrupted_correct_answer'], task_data['corrupted_incorrect_answer'])\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}'\")\n",
    "    print(f\"  - Clean Run Baseline Logit Diff: {clean_baseline:.4f}\")\n",
    "    print(f\"  - Corrupted Run Baseline Logit Diff: {corrupted_baseline:.4f}\")\n",
    "\n",
    "    results = []\n",
    "    print(\"  - Patching Attention Heads...\")\n",
    "    for layer in range(n_layers):\n",
    "        for head in range(n_heads):\n",
    "            patched_logit_diff = perform_patching_experiment(\n",
    "                model, tokenizer, source_prompt, dest_prompt,\n",
    "                dest_correct_answer, dest_incorrect_answer,\n",
    "                layer, 'attn_head', head\n",
    "            )\n",
    "            results.append({'layer': layer, 'head': head, 'type': 'attn_head', 'patched_logit_diff': patched_logit_diff})\n",
    "\n",
    "    print(\"  - Patching MLP Layers...\")\n",
    "    for layer in range(n_layers):\n",
    "        patched_logit_diff = perform_patching_experiment(\n",
    "            model, tokenizer, source_prompt, dest_prompt,\n",
    "            dest_correct_answer, dest_incorrect_answer,\n",
    "            layer, 'mlp'\n",
    "        )\n",
    "        results.append({'layer': layer, 'head': -1, 'type': 'mlp', 'patched_logit_diff': patched_logit_diff})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if patch_type == 'noising':\n",
    "        df['effect'] = df['patched_logit_diff'] - clean_baseline\n",
    "    else:\n",
    "        df['effect'] = df['patched_logit_diff'] - corrupted_baseline\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7237ef8a08bf1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    \"\"\"Plots a heatmap or bar chart using Matplotlib and optionally saves it.\"\"\"\n",
    "    if component_type == 'attn_head':\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='head', columns='layer', values='effect')\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        # Center the color map around 0\n",
    "        max_abs_val = pivot_df.abs().max().max()\n",
    "        im = ax.imshow(pivot_df, cmap='coolwarm', vmin=-max_abs_val, vmax=max_abs_val, aspect='auto')\n",
    "        # Create colorbar\n",
    "        cbar = ax.figure.colorbar(im, ax=ax)\n",
    "        cbar.ax.set_ylabel(\"Effect on Logit Difference\", rotation=-90, va=\"bottom\")\n",
    "        # Set ticks and labels\n",
    "        ax.set_xticks(np.arange(pivot_df.shape[1]))\n",
    "        ax.set_yticks(np.arange(pivot_df.shape[0]))\n",
    "        ax.set_xticklabels(pivot_df.columns)\n",
    "        ax.set_yticklabels(pivot_df.index)\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Head Index\")\n",
    "    elif component_type == 'mlp':\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(mlp_df['layer'], mlp_df['effect'], color='skyblue')\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Average Effect on Logit Difference\")\n",
    "        ax.grid(axis='y', linestyle='--')\n",
    "        # Set x-ticks to be integers\n",
    "        ax.set_xticks(mlp_df['layer'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid component type\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf')\n",
    "        print(f\"Saved plot to: {output_path}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7aa16581ade1ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer for 'gpt2'...\n",
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Clean Run Baseline Logit Diff: 2.0557\n",
      "  - Corrupted Run Baseline Logit Diff: -2.3486\n",
      "  - Patching Attention Heads...\n",
      "  - Patching MLP Layers...\n",
      "\n",
      "Running denoising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Clean Run Baseline Logit Diff: 2.0557\n",
      "  - Corrupted Run Baseline Logit Diff: -2.3486\n",
      "  - Patching Attention Heads...\n",
      "  - Patching MLP Layers...\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task'\n",
      "  - Clean Run Baseline Logit Diff: -0.1954\n",
      "  - Corrupted Run Baseline Logit Diff: 0.0659\n",
      "  - Patching Attention Heads...\n",
      "  - Patching MLP Layers...\n",
      "\n",
      "Running denoising sweep for 'Next-Item Sequencing Task'\n",
      "  - Clean Run Baseline Logit Diff: -0.1954\n",
      "  - Corrupted Run Baseline Logit Diff: 0.0659\n",
      "  - Patching Attention Heads...\n",
      "  - Patching MLP Layers...\n",
      "\n",
      "--- Saving Results to CSV ---\n",
      "Saved data to: patching_results_data_3/analogy_noising_results.csv\n",
      "Saved data to: patching_results_data_3/analogy_denoising_results.csv\n",
      "Saved data to: patching_results_data_3/sequencing_noising_results.csv\n",
      "Saved data to: patching_results_data_3/sequencing_denoising_results.csv\n",
      "\n",
      "--- Generating and Saving Plots ---\n",
      "Saved plot to: patching_results_plots_3/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: patching_results_plots_3/analogy_noising_mlp_layers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: patching_results_plots_3/analogy_denoising_attn_heads.pdf\n",
      "Saved plot to: patching_results_plots_3/analogy_denoising_mlp_layers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: patching_results_plots_3/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: patching_results_plots_3/sequencing_noising_mlp_layers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: patching_results_plots_3/sequencing_denoising_attn_heads.pdf\n",
      "Saved plot to: patching_results_plots_3/sequencing_denoising_mlp_layers.pdf\n",
      "\n",
      "--- End of Experiment ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/scratch-local/awadehra.12342160/ipykernel_2627508/3715323595.py:36: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    MODEL_NAME = 'gpt2'\n",
    "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for task_name, task_data in datasets.items():\n",
    "        for patch_type in ['noising', 'denoising']:\n",
    "            result_key = f\"{task_name}_{patch_type}\"\n",
    "            df = run_exploratory_sweep(model, tokenizer, task_data, patch_type)\n",
    "            all_results[result_key] = df\n",
    "\n",
    "    # --- Save Dataframes to CSV ---\n",
    "    print(\"\\n--- Saving Results to CSV ---\")\n",
    "    output_dir_data = \"patching_results_data_3\"\n",
    "    if not os.path.exists(output_dir_data):\n",
    "        os.makedirs(output_dir_data)\n",
    "\n",
    "    for result_key, df in all_results.items():\n",
    "        file_path = os.path.join(output_dir_data, f\"{result_key}_results.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved data to: {file_path}\")\n",
    "\n",
    "    # --- Visualize and Save Plots to PDF ---\n",
    "    print(\"\\n--- Generating and Saving Plots ---\")\n",
    "    output_dir_plots = \"patching_results_plots_3\"\n",
    "    if not os.path.exists(output_dir_plots):\n",
    "        os.makedirs(output_dir_plots)\n",
    "\n",
    "    for result_key, df in all_results.items():\n",
    "        task_name, patch_type = result_key.split('_')\n",
    "\n",
    "        plot_path_attn = os.path.join(output_dir_plots, f\"{result_key}_attn_heads.pdf\")\n",
    "        plot_path_mlp = os.path.join(output_dir_plots, f\"{result_key}_mlp_layers.pdf\")\n",
    "\n",
    "        title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{datasets[task_name]['description']}\"\n",
    "        plot_results(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "\n",
    "        title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{datasets[task_name]['description']}\"\n",
    "        plot_results(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "    print(\"\\n--- End of Experiment ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
