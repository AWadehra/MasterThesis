{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Dict, Callable"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def setup_model_and_tokenizer(model_name: str = 'gpt2'):\n",
    "    \"\"\"\n",
    "    Loads a pretrained GPT-2 model and its tokenizer.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model and tokenizer for '{model_name}'...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer"
   ],
   "id": "17e8fe4c32f734fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Defines the clean and corrupted prompts and answers for each task.\n",
    "    \"\"\"\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs 'Swap' Rule)\",\n",
    "            \"clean_prompt\": \"ABCD:ABCE::JKLM:JKL\",\n",
    "            \"clean_correct_answer\": \"N\",\n",
    "            \"clean_incorrect_answer\": \"M\",\n",
    "            \"corrupted_prompt\": \"ABCD:BACD::JKLM:KJL\",\n",
    "            \"corrupted_correct_answer\": \"M\",\n",
    "            \"corrupted_incorrect_answer\": \"N\",\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"clean_prompt\": \"January, February, March, April,\",\n",
    "            \"clean_correct_answer\": \" May\",\n",
    "            \"clean_incorrect_answer\": \" July\",\n",
    "            \"corrupted_prompt\": \"January, Car, Plane, April,\",\n",
    "            \"corrupted_correct_answer\": \" May\",\n",
    "            \"corrupted_incorrect_answer\": \" July\",\n",
    "        }\n",
    "    }\n",
    "    return datasets"
   ],
   "id": "74224b1c7fca6ba3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "activation_cache: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "def caching_hook_factory(hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        activation_cache[hook_name] = tensor_to_cache.detach()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in activation_cache:\n",
    "            raise ValueError(f\"Activation for {hook_name} not found in cache!\")\n",
    "        cached_activation = activation_cache[hook_name]\n",
    "        if head_index is not None:\n",
    "            original_output = output[0]\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            patched_output = original_output.clone()\n",
    "            patched_output[:, :, start:end] = cached_activation[:, :, start:end]\n",
    "            return (patched_output, output[1])\n",
    "        else:\n",
    "            return cached_activation\n",
    "    return hook"
   ],
   "id": "45e58773797e47a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'):\n",
    "        model = getattr(model, part)\n",
    "    return model"
   ],
   "id": "3479e78a1b69d0c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_with_hooks(\n",
    "        model: nn.Module,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        prompt: str,\n",
    "        hook_fns: Dict[str, Callable],\n",
    ") -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module_name = name.split('_')[0] if 'attn.c_proj' in name else name\n",
    "            module = get_module_by_name(model, module_name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()"
   ],
   "id": "4df82e128c02df33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_logit_diff(\n",
    "        logits: torch.Tensor,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        correct_answer: str,\n",
    "        incorrect_answer: str\n",
    ") -> float:\n",
    "    correct_id = tokenizer.encode(correct_answer, add_prefix_space=False)[0]\n",
    "    incorrect_id = tokenizer.encode(incorrect_answer, add_prefix_space=False)[0]\n",
    "    return (logits[correct_id] - logits[incorrect_id]).ite"
   ],
   "id": "d7b673729edf14f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def perform_patching_experiment(\n",
    "        model: nn.Module,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        source_prompt: str,\n",
    "        dest_prompt: str,\n",
    "        dest_correct_answer: str,\n",
    "        dest_incorrect_answer: str,\n",
    "        layer: int,\n",
    "        component_type: str,\n",
    "        head_index: int = None\n",
    ") -> float:\n",
    "    global activation_cache\n",
    "    activation_cache = {}\n",
    "\n",
    "    if component_type == 'attn_head':\n",
    "        hook_name = f\"transformer.h.{layer}.attn.c_proj\"\n",
    "    elif component_type == 'mlp':\n",
    "        hook_name = f\"transformer.h.{layer}.mlp.c_proj\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid component type\")\n",
    "\n",
    "    caching_hooks = {hook_name: caching_hook_factory(hook_name)}\n",
    "    run_with_hooks(model, tokenizer, source_prompt, caching_hooks)\n",
    "\n",
    "    d_head = model.config.n_embd // model.config.n_head if component_type == 'attn_head' else None\n",
    "    patching_hooks = {hook_name: patching_hook_factory(hook_name, head_index, d_head)}\n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(\n",
    "        model: nn.Module,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        task_data: Dict,\n",
    "        patch_type: str\n",
    ") -> pd.DataFrame:\n",
    "    n_layers = model.config.n_layer\n",
    "    n_heads = model.config.n_head\n",
    "\n",
    "    if patch_type == 'noising':\n",
    "        source_prompt = task_data['corrupted_prompt']\n",
    "        dest_prompt = task_data['clean_prompt']\n",
    "        dest_correct_answer = task_data['clean_correct_answer']\n",
    "        dest_incorrect_answer = task_data['clean_incorrect_answer']\n",
    "    elif patch_type == 'denoising':\n",
    "        source_prompt = task_data['clean_prompt']\n",
    "        dest_prompt = task_data['corrupted_prompt']\n",
    "        dest_correct_answer = task_data['corrupted_correct_answer']\n",
    "        dest_incorrect_answer = task_data['corrupted_incorrect_answer']\n",
    "    else:\n",
    "        raise ValueError(\"patch_type must be 'noising' or 'denoising'\")\n",
    "\n",
    "    clean_logits = run_with_hooks(model, tokenizer, task_data['clean_prompt'], {})\n",
    "    clean_baseline = calculate_logit_diff(clean_logits, tokenizer, task_data['clean_correct_answer'], task_data['clean_incorrect_answer'])\n",
    "\n",
    "    corrupted_logits = run_with_hooks(model, tokenizer, task_data['corrupted_prompt'], {})\n",
    "    corrupted_baseline = calculate_logit_diff(corrupted_logits, tokenizer, task_data['corrupted_correct_answer'], task_data['corrupted_incorrect_answer'])\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}'\")\n",
    "    print(f\"  - Clean Run Baseline Logit Diff: {clean_baseline:.4f}\")\n",
    "    print(f\"  - Corrupted Run Baseline Logit Diff: {corrupted_baseline:.4f}\")\n",
    "\n",
    "    results = []\n",
    "    print(\"  - Patching Attention Heads...\")\n",
    "    for layer in range(n_layers):\n",
    "        for head in range(n_heads):\n",
    "            patched_logit_diff = perform_patching_experiment(\n",
    "                model, tokenizer, source_prompt, dest_prompt,\n",
    "                dest_correct_answer, dest_incorrect_answer,\n",
    "                layer, 'attn_head', head\n",
    "            )\n",
    "            results.append({'layer': layer, 'head': head, 'type': 'attn_head', 'patched_logit_diff': patched_logit_diff})\n",
    "\n",
    "    print(\"  - Patching MLP Layers...\")\n",
    "    for layer in range(n_layers):\n",
    "        patched_logit_diff = perform_patching_experiment(\n",
    "            model, tokenizer, source_prompt, dest_prompt,\n",
    "            dest_correct_answer, dest_incorrect_answer,\n",
    "            layer, 'mlp'\n",
    "        )\n",
    "        results.append({'layer': layer, 'head': -1, 'type': 'mlp', 'patched_logit_diff': patched_logit_diff})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if patch_type == 'noising':\n",
    "        df['effect'] = df['patched_logit_diff'] - clean_baseline\n",
    "    else:\n",
    "        df['effect'] = df['patched_logit_diff'] - corrupted_baseline\n",
    "\n",
    "    return df"
   ],
   "id": "10818139b27878ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_heatmap(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap or bar chart and optionally saves it to a file.\n",
    "    \"\"\"\n",
    "    if component_type == 'attn_head':\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='layer', columns='head', values='effect')\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(pivot_df, cmap='coolwarm', center=0.0, annot=False)\n",
    "        plt.xlabel(\"Head Index\")\n",
    "        plt.ylabel(\"Layer\")\n",
    "    elif component_type == 'mlp':\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='layer', y='effect', data=mlp_df, color='skyblue')\n",
    "        plt.xlabel(\"Layer\")\n",
    "        plt.ylabel(\"Effect on Logit Difference\")\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid component type for plotting\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf')\n",
    "        print(f\"Saved plot to: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "id": "7237ef8a08bf1cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    MODEL_NAME = 'gpt2'\n",
    "    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for task_name, task_data in datasets.items():\n",
    "        for patch_type in ['noising', 'denoising']:\n",
    "            result_key = f\"{task_name}_{patch_type}\"\n",
    "            df = run_exploratory_sweep(model, tokenizer, task_data, patch_type)\n",
    "            all_results[result_key] = df\n",
    "\n",
    "    # --- Save Dataframes to CSV ---\n",
    "    print(\"\\n--- Saving Results to CSV ---\")\n",
    "    output_dir_data = \"patching_results_data\"\n",
    "    if not os.path.exists(output_dir_data):\n",
    "        os.makedirs(output_dir_data)\n",
    "\n",
    "    for result_key, df in all_results.items():\n",
    "        file_path = os.path.join(output_dir_data, f\"{result_key}_results.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved data to: {file_path}\")\n",
    "\n",
    "    # --- Visualize and Save Plots to PDF ---\n",
    "    print(\"\\n--- Generating and Saving Plots ---\")\n",
    "    output_dir_plots = \"patching_results_plots\"\n",
    "    if not os.path.exists(output_dir_plots):\n",
    "        os.makedirs(output_dir_plots)\n",
    "\n",
    "    for result_key, df in all_results.items():\n",
    "        task_name, patch_type = result_key.split('_')\n",
    "\n",
    "        plot_path_attn = os.path.join(output_dir_plots, f\"{result_key}_attn_heads.pdf\")\n",
    "        plot_path_mlp = os.path.join(output_dir_plots, f\"{result_key}_mlp_layers.pdf\")\n",
    "\n",
    "        title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{datasets[task_name]['description']}\"\n",
    "        plot_heatmap(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "\n",
    "        title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{datasets[task_name]['description']}\"\n",
    "        plot_heatmap(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "    print(\"\\n--- End of Experiment ---\")"
   ],
   "id": "7aa16581ade1ba08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
