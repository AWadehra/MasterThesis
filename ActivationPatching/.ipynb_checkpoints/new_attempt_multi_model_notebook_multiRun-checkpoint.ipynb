{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: No module named 'requests'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnn\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.11/site-packages/transformers/__init__.py:26\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     28\u001B[39m     OptionalDependencyNotAvailable,\n\u001B[32m     29\u001B[39m     _LazyModule,\n\u001B[32m   (...)\u001B[39m\u001B[32m     48\u001B[39m     logging,\n\u001B[32m     49\u001B[39m )\n\u001B[32m     52\u001B[39m logger = logging.get_logger(\u001B[34m__name__\u001B[39m)  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.11/site-packages/transformers/dependency_versions_check.py:16\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdependency_versions_table\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mversions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[32m     25\u001B[39m pkgs_to_check_at_runtime = [\n\u001B[32m     26\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     27\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtqdm\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     37\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mpyyaml\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     38\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.11/site-packages/transformers/utils/__init__.py:19\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#!/usr/bin/env python\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfunctools\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhuggingface_hub\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_full_repo_name  \u001B[38;5;66;03m# for backward compatibility\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhuggingface_hub\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconstants\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HF_HUB_DISABLE_TELEMETRY \u001B[38;5;28;01mas\u001B[39;00m DISABLE_TELEMETRY  \u001B[38;5;66;03m# for backward compatibility\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpackaging\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m version\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.11/site-packages/huggingface_hub/__init__.py:972\u001B[39m, in \u001B[36m_attach.<locals>.__getattr__\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m    970\u001B[39m submod_path = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpackage_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_to_modules[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    971\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m972\u001B[39m     submod = \u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubmod_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    974\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError importing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubmod_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib64/python3.11/importlib/__init__.py:126\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m    124\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    125\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.11/site-packages/huggingface_hub/hf_api.py:47\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     31\u001B[39m     Any,\n\u001B[32m     32\u001B[39m     BinaryIO,\n\u001B[32m   (...)\u001B[39m\u001B[32m     43\u001B[39m     overload,\n\u001B[32m     44\u001B[39m )\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01murllib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mparse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m quote, unquote\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrequests\u001B[39;00m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrequests\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexceptions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HTTPError\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtqdm\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mauto\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;28;01mas\u001B[39;00m base_tqdm\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Callable, List\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c1433-df4e-4e5f-bdfd-1f726046072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 1: ADD DETERMINISM FOUNDATION =====\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "# ===== END CHANGE 1 ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de925cccebe491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(model: nn.Module, model_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a standardized configuration dictionary for various model architectures.\n",
    "    This provides the correct hook names needed for our specific patching experiment.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"\n",
    "        })\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\",\n",
    "            \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"\n",
    "        })\n",
    "    elif 'llama' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\",\n",
    "            \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"\n",
    "        })\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model architecture for '{model_name}' not recognized. Please add its configuration.\")\n",
    "\n",
    "    return config\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Loads a pretrained Hugging Face model and tokenizer, handling various architectures.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    # --- ADD HUGGING FACE TOKEN HERE FOR GATED MODELS LIKE LLAMA ---\n",
    "    # Replace \"YOUR_HF_TOKEN_HERE\" with your actual token.\n",
    "    # It can be a read-only token for security.\n",
    "    HUGGING_FACE_TOKEN = \"hf_findNewOne\"\n",
    "\n",
    "    model_dtype = torch.float16 if any(k in model_name.lower() for k in ['6b', '13b', '20b', '70b']) else torch.float32\n",
    "\n",
    "    if 'llama' in model_name.lower():\n",
    "        if HUGGING_FACE_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "            print(\"Warning: Llama model selected, but no Hugging Face token provided. This may fail.\")\n",
    "            access_token = None\n",
    "        else:\n",
    "            access_token = HUGGING_FACE_TOKEN\n",
    "\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True, token=access_token).to(device)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # ===== CHANGE 6: LAYERNORM STABILIZATION =====\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.LayerNorm):\n",
    "            module.eps = 1e-3  # Increased from 1e-5/1e-6\n",
    "    # ===== END CHANGE 6 =====\n",
    "    \n",
    "    model_config = get_model_config(model, model_name)\n",
    "\n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8c5e770643139b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\n",
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt2' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neo-125m' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_heads,\n",
    "                      \"n_layers\":model.config.num_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neo.layers.{layer}.attention.dense' for layer in range(model.config.num_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neo.layers.{layer}' for layer in range(model.config.num_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower() or 'pythia' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neox.layers.{layer}.attention.dense' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neox.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            access_token = \"hf_findNewOne\"\n",
    "\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                token=access_token\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower() or '8b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else: #half precision for bigger llama models\n",
    "                #This becomes only for the 13B model then. Okay then. What else?\n",
    "                model_dtype = torch.float16\n",
    "\n",
    "            # If transformers version is < 4.31 use LlamaLoaders\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            # model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "            # If transformers version is >= 4.31, use AutoLoaders\n",
    "            access_token = \"hf_findNewOne\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, token=access_token).to(device)\n",
    "\n",
    "        # --- THIS IS THE FIX ---\n",
    "        if tokenizer.pad_token is None:\n",
    "            print(\"Llama tokenizer does not have a pad token. Setting pad_token = eos_token.\")\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = model.config.eos_token_id\n",
    "        \n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\":model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config._name_or_path,\n",
    "                      \"attn_hook_names\":[f'model.layers.{layer}.self_attn.o_proj' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'model.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":True}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e81171-06f0-4c34-8a1f-ef91dbdff202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer. This version is corrected to\n",
    "    handle all model architectures and create the correct config dictionary.\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    HUGGING_FACE_TOKEN = \"hf_findNewOne\" # User's provided token\n",
    "    kwargs = {'low_cpu_mem_usage': True}\n",
    "    \n",
    "    # --- Model Loading Logic ---\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        print(\"Using float16 revision for gpt-j-6B to ensure PyTorch-only workflow.\")\n",
    "        kwargs['revision'] = 'float16'\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    elif 'llama' in model_name.lower():\n",
    "        access_token = HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != \"YOUR_HF_TOKEN_HERE\" else None\n",
    "        if not access_token: print(\"Warning: Llama model selected, but no Hugging Face token provided.\")\n",
    "        kwargs['token'] = access_token\n",
    "        if '70b' in model_name.lower():\n",
    "            bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "            kwargs['quantization_config'] = bnb_config\n",
    "            kwargs['trust_remote_code'] = True\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "        else:\n",
    "            kwargs['torch_dtype'] = torch.float16 if any(k in model_name.lower() for k in ['13b']) else torch.float32\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16 if '20b' in model_name.lower() else torch.float32\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # --- THIS IS THE FIX: Create the config dict with the correct keys ---\n",
    "    model_config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"})\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\", \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"})\n",
    "    elif 'llama' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\", \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"})\n",
    "    else: raise NotImplementedError(\"Model architecture not recognized.\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca8a232f4254d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    \"\"\"Defines the datasets for each task, loading and formatting from JSON files.\"\"\"\n",
    "    \n",
    "    # ===== CHANGE 2: SEED PROMPT SAMPLING =====\n",
    "    random.seed(SEED)\n",
    "    # ===== END CHANGE 2 =====\n",
    "    \n",
    "    # --- Analogy Task Dataset Generation ---\n",
    "    with open('../BaselineAccuracy/dataset_files/abstractive/succ_letterstring_basic.json', 'r') as f:\n",
    "        analogy_data = json.load(f)\n",
    "\n",
    "    analogy_prompts = []\n",
    "    # Take a random sample of 20 to ensure variety each time the script is run\n",
    "    for _ in range(20):\n",
    "        # Ensure the example and target prompts are different\n",
    "        # ===== CHANGE 2: USE CHOICE INSTEAD OF SAMPLE =====\n",
    "        while True:\n",
    "            example_pair, target_pair = random.sample(analogy_data, 2)\n",
    "            if example_pair['input'] != target_pair['input']:\n",
    "                break\n",
    "        # ===== END CHANGE 2 =====\n",
    "\n",
    "        # Clean up the strings from the JSON file and format them\n",
    "        # e.g., \"[e f g h]\" -> \"e:f:g:h\"\n",
    "        example_in = example_pair['input'].strip('[]').replace(' ', '')\n",
    "        example_out = example_pair['output'].strip('[]').replace(' ', '')\n",
    "        target_in_full = target_pair['input'].strip('[]').replace(' ', '')\n",
    "        target_out_full = target_pair['output'].strip('[]').replace(' ', '')\n",
    "\n",
    "        # Create the prefix and single-token answers\n",
    "        #target_in_parts = target_in_full.split(':')\n",
    "        target_prefix = target_out_full[:3]\n",
    "        correct_answer = target_out_full[-1]\n",
    "        incorrect_answer = target_in_full[-1]\n",
    "\n",
    "        analogy_prompts.append({\n",
    "            \"clean_prompt\": f\"{example_in}:{example_out}::{target_in_full}:{target_prefix}\",\n",
    "            \"clean_correct_answer\": correct_answer,\n",
    "            \"clean_incorrect_answer\": incorrect_answer,\n",
    "            \"corrupted_prompt\": f\"{example_in}:{example_in}::{target_in_full}:{target_prefix}\",\n",
    "            \"corrupted_correct_answer\": incorrect_answer,\n",
    "            \"corrupted_incorrect_answer\": correct_answer,\n",
    "        })\n",
    "\n",
    "    # --- Sequencing Task Dataset Generation ---\n",
    "    with open('../BaselineAccuracy/dataset_files/abstractive/next_item.json', 'r') as f:\n",
    "        sequencing_data = json.load(f)\n",
    "\n",
    "    sequencing_prompts = []\n",
    "    # Take a random sample of 20\n",
    "    for _ in range(20):\n",
    "        # ===== CHANGE 2: USE CHOICE INSTEAD OF SAMPLE =====\n",
    "        while True:\n",
    "            example_pair, target_pair = random.sample(sequencing_data, 2)\n",
    "            if example_pair['input'] != target_pair['input']:\n",
    "                break\n",
    "        # ===== END CHANGE 2 =====\n",
    "\n",
    "        # Add leading spaces to ensure consistent tokenization\n",
    "        example_in, example_out =  example_pair['input'], example_pair['output']\n",
    "        target_in, target_out = target_pair['input'], target_pair['output']\n",
    "\n",
    "        sequencing_prompts.append({\n",
    "            \"clean_prompt\": f\"{example_in}:{example_out}::{target_in}:\",\n",
    "            \"clean_correct_answer\": target_out,\n",
    "            \"clean_incorrect_answer\": target_in,\n",
    "            \"corrupted_prompt\": f\"{example_in}:{example_in}::{target_in}:\",\n",
    "            \"corrupted_correct_answer\": target_in,\n",
    "            \"corrupted_incorrect_answer\": target_out,\n",
    "        })\n",
    "\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs No Rule)\",\n",
    "            \"prompts\": analogy_prompts\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"prompts\": sequencing_prompts\n",
    "        }\n",
    "    }\n",
    "    #print(datasets)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5ed801d717061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 4: LOCAL CACHE ISOLATION =====\n",
    "def caching_hook_factory(cache: dict, hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        cache[hook_name] = tensor_to_cache.detach().clone()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(cache: dict, hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in cache:\n",
    "            raise ValueError(f\"Activation for {hook_name} not found!\")\n",
    "        cached_activation = cache[hook_name]\n",
    "        patched_output = output.clone()\n",
    "        min_seq_len = min(patched_output.shape[-2], cached_activation.shape[-2])\n",
    "        if head_index is not None:\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            if patched_output.ndim == 3: \n",
    "                patched_output[:, :min_seq_len, start:end] = cached_activation[:, :min_seq_len, start:end]\n",
    "            elif patched_output.ndim == 2: \n",
    "                patched_output[:min_seq_len, start:end] = cached_activation[:min_seq_len, start:end]\n",
    "        else:\n",
    "            if patched_output.ndim == 3: \n",
    "                patched_output[:, :min_seq_len, :] = cached_activation[:, :min_seq_len, :]\n",
    "            elif patched_output.ndim == 2: \n",
    "                patched_output[:min_seq_len, :] = cached_activation[:min_seq_len, :]\n",
    "        return patched_output\n",
    "    return hook\n",
    "# ===== END CHANGE 4 =====\n",
    "\n",
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'): model = getattr(model, part)\n",
    "    return model\n",
    "\n",
    "def run_with_hooks(model: nn.Module, tokenizer: AutoTokenizer, prompt: str, hook_fns: Dict[str, Callable]) -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module = get_module_by_name(model, name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "            \n",
    "        # ===== CHANGE 7: ATTENTION MASK ENFORCEMENT =====\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move ALL to device\n",
    "        # ===== END CHANGE 7 =====\n",
    "        \n",
    "        with torch.no_grad(): outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles: handle.remove()\n",
    "\n",
    "def calculate_logit_diff(logits: torch.Tensor, tokenizer: AutoTokenizer, correct_answer: str, incorrect_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the logit difference.\n",
    "    FIX: Uses `add_special_tokens=False` to prevent the tokenizer from adding\n",
    "    a Beginning-Of-Sentence token, which would make the logit difference always zero.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ===== CHANGE 3: TOKENIZATION SAFETY =====\n",
    "        correct_tokens = tokenizer.encode(correct_answer, add_special_tokens=False)\n",
    "        incorrect_tokens = tokenizer.encode(incorrect_answer, add_special_tokens=False)\n",
    "        \n",
    "        if not correct_tokens or not incorrect_tokens:\n",
    "            return 0.0\n",
    "            \n",
    "        correct_id = correct_tokens[0]\n",
    "        incorrect_id = incorrect_tokens[0]\n",
    "        # ===== END CHANGE 3 =====\n",
    "\n",
    "        return (logits[correct_id] - logits[incorrect_id]).item()\n",
    "\n",
    "    except IndexError:\n",
    "        # This can happen if the tokenizer returns an empty list for a given string\n",
    "        print(f\"Warning: Tokenizer failed to encode '{correct_answer}' or '{incorrect_answer}'.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in calculate_logit_diff: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32dcfdfe06c5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_patching_experiment(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, source_prompt: str, dest_prompt: str, dest_correct_answer: str, dest_incorrect_answer: str, layer: int, component_type: str, head_index: int = None) -> float:\n",
    "   # ===== CHANGE 4: LOCAL CACHE ISOLATION =====\n",
    "    local_cache = {}\n",
    "    # ===== END CHANGE 4 =====\n",
    "    hook_template = model_config['mlp_hook_name_template'] if component_type == 'mlp' else model_config['attn_hook_name_template']\n",
    "    hook_name = hook_template.format(layer)\n",
    "    \n",
    "    # ===== CHANGE 4: USE LOCAL CACHE =====\n",
    "    run_with_hooks(model, tokenizer, source_prompt, \n",
    "                  {hook_name: caching_hook_factory(local_cache, hook_name)})\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    d_head = model_config[\"d_model\"] // model_config[\"n_heads\"] if component_type == 'attn_head' else None\n",
    "    \n",
    "    # ===== CHANGE 4: USE LOCAL CACHE =====\n",
    "    patching_hooks = {hook_name: patching_hook_factory(local_cache, hook_name, head_index, d_head)}\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, task_data: Dict, patch_type: str) -> pd.DataFrame:\n",
    "    n_layers, n_heads = model_config[\"n_layers\"], model_config[\"n_heads\"]\n",
    "    prompt_dataset = task_data['prompts']\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}' over {len(prompt_dataset)} prompts...\")\n",
    "\n",
    "    # Pre-calculate all baseline scores for efficiency\n",
    "    clean_baselines, corrupted_baselines = [], []\n",
    "    for prompt_set in prompt_dataset:\n",
    "        clean_logits = run_with_hooks(model, tokenizer, prompt_set['clean_prompt'], {})\n",
    "        clean_baselines.append(calculate_logit_diff(clean_logits, tokenizer, prompt_set['clean_correct_answer'], prompt_set['clean_incorrect_answer']))\n",
    "        corrupted_logits = run_with_hooks(model, tokenizer, prompt_set['corrupted_prompt'], {})\n",
    "        corrupted_baselines.append(calculate_logit_diff(corrupted_logits, tokenizer, prompt_set['corrupted_correct_answer'], prompt_set['corrupted_incorrect_answer']))\n",
    "\n",
    "    results = []\n",
    "    for component_type, head_range in [('attn_head', range(n_heads)), ('mlp', [-1])]:\n",
    "        print(f\"  - Patching {component_type}s...\")\n",
    "        for layer in range(n_layers):\n",
    "            for head_index in head_range:\n",
    "                effects_for_this_component = []\n",
    "                # Inner loop to iterate over the dataset for each component\n",
    "                for i, prompt_set in enumerate(prompt_dataset):\n",
    "                    global activation_cache\n",
    "                    activation_cache = {}\n",
    "                    if patch_type == 'noising':\n",
    "                        source_prompt, dest_prompt = prompt_set['corrupted_prompt'], prompt_set['clean_prompt']\n",
    "                        dest_correct, dest_incorrect = prompt_set['clean_correct_answer'], prompt_set['clean_incorrect_answer']\n",
    "                        baseline_to_compare = clean_baselines[i]\n",
    "                    else: # denoising\n",
    "                        source_prompt, dest_prompt = prompt_set['clean_prompt'], prompt_set['corrupted_prompt']\n",
    "                        dest_correct, dest_incorrect = prompt_set['corrupted_correct_answer'], prompt_set['corrupted_incorrect_answer']\n",
    "                        baseline_to_compare = corrupted_baselines[i]\n",
    "\n",
    "                    # Perform the patching experiment for this single prompt\n",
    "                    patched_logit_diff = perform_patching_experiment(model, tokenizer, model_config, source_prompt, dest_prompt, dest_correct, dest_incorrect, layer, component_type, head_index if component_type == 'attn_head' else None)\n",
    "                    effect = patched_logit_diff - baseline_to_compare\n",
    "                    effects_for_this_component.append(effect)\n",
    "\n",
    "                # Calculate the average effect across all prompts for this one component\n",
    "                average_effect = np.mean(effects_for_this_component)\n",
    "                results.append({'layer': layer, 'head': head_index, 'type': component_type, 'effect': average_effect})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3467394fb471dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    if component_type == 'attn_head':\n",
    "        if df[df['type'] == 'attn_head'].empty: return\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='head', columns='layer', values='effect')\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        max_abs_val = pivot_df.abs().max().max() if not pivot_df.empty else 1.0\n",
    "        im = ax.imshow(pivot_df, cmap='coolwarm', vmin=-max_abs_val, vmax=max_abs_val, aspect='auto')\n",
    "        cbar = ax.figure.colorbar(im, ax=ax); cbar.ax.set_ylabel(\"Effect on Logit Difference\", rotation=-90, va=\"bottom\")\n",
    "        ax.set_xticks(np.arange(pivot_df.shape[1])); ax.set_yticks(np.arange(pivot_df.shape[0]))\n",
    "        ax.set_xticklabels(pivot_df.columns); ax.set_yticklabels(pivot_df.index)\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Head Index\")\n",
    "    elif component_type == 'mlp':\n",
    "        if df[df['type'] == 'mlp'].empty: return\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(mlp_df['layer'], mlp_df['effect'], color='skyblue')\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Average Effect on Logit Difference\")\n",
    "        ax.grid(axis='y', linestyle='--'); ax.set_xticks(mlp_df['layer'])\n",
    "    else: raise ValueError(\"Invalid component type\")\n",
    "    ax.set_title(title); fig.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf'); print(f\"Saved plot to: {output_path}\")\n",
    "    plt.show(); plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bcdfdc6df219ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the activation patching experiment across multiple models.\n",
    "    \"\"\"\n",
    "    # This model dictionary is taken directly from the user's script\n",
    "    models_to_test = {\n",
    "        #'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        #'gpt2': 'gpt2', # Added gpt2 for a quick baseline\n",
    "        #'gptj6b': 'EleutherAI/gpt-j-6B',\n",
    "        #'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "        'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "        #'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "        #'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    main_output_dir = \"Results_5_Random_MultiRun\"\n",
    "    os.makedirs(main_output_dir, exist_ok=True)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    for model_short_name, model_hf_name in models_to_test.items():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_hf_name)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- Could not load model {model_hf_name}. Skipping. Error: {e} ---\\n\")\n",
    "            \n",
    "            from transformers import file_utils\n",
    "            import shutil\n",
    "            import re\n",
    "    \n",
    "            # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "            model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "            cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "    \n",
    "            # 2. Delete only this model's folder\n",
    "            if os.path.exists(cache_path):\n",
    "                print(f\"Deleting model cache: {cache_path}\")\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "                \n",
    "            continue\n",
    "\n",
    "        # Create model-specific subdirectory inside the main \"Results\" folder\n",
    "        model_results_dir = os.path.join(main_output_dir, model_short_name)\n",
    "        os.makedirs(model_results_dir, exist_ok=True)\n",
    "\n",
    "        for task_name, task_data in datasets.items():\n",
    "            for patch_type in ['noising']:\n",
    "                result_key = f\"{task_name}_{patch_type}\"\n",
    "                df = run_exploratory_sweep(model, tokenizer, model_config, task_data, patch_type)\n",
    "\n",
    "                # Save CSV and Plots inside the model-specific subfolder\n",
    "                csv_path = os.path.join(model_results_dir, f\"{result_key}_results.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved data to: {csv_path}\")\n",
    "\n",
    "                plot_path_attn = os.path.join(model_results_dir, f\"{result_key}_attn_heads.pdf\")\n",
    "                plot_path_mlp = os.path.join(model_results_dir, f\"{result_key}_mlp_layers.pdf\")\n",
    "                title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                plot_results(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "                plot_results(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "        print(f\"--- Finished with {model_short_name}. Clearing memory. ---\")\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # --- NEW: Delete ONLY this model's cache ---\n",
    "        from transformers import file_utils\n",
    "        import shutil\n",
    "        import re\n",
    "\n",
    "        # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "        model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "        cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "\n",
    "        # 2. Delete only this model's folder\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Deleting model cache: {cache_path}\")\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4650f51bc272a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'gpt2' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a7e1e5276d452093f65aa5e9def9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf4565cdfc74bd89a3f839a4082cdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6424cc827ede464abddb48f32c003c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de41e7de4c149e3979313aa4c30ed77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc35d7939b249b0bb89777575f22d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51124a4d15c47fc939c0c245e80c8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa4bb9f008e41ee8ccad6917575af90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/gpt2/analogy_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/gpt2/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/gpt2/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task' over 20 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/gpt2/sequencing_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/gpt2/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/gpt2/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with gpt2. Clearing memory. ---\n",
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--gpt2\n",
      "--- Loading model and tokenizer for 'EleutherAI/gpt-j-6B' ---\n",
      "Using float16 revision for gpt-j-6B to ensure PyTorch-only workflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b9d8a6be2e43fe9c3c0e6d58454251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c850f3b666b74d2e91805385b795f7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181200fca2074b7caf754c429bc5aa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7267b1faefb45168cab23f2622f21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b3ea05345c4753a72725292985a7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba0b42933984c779fe78b67ec8cfec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a0bb58f2224e3389130f721c70547e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/836 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1cf8955a8c4978b4e6a296314dadea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/gptj6b/analogy_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/gptj6b/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/gptj6b/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task' over 20 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/gptj6b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/gptj6b/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/gptj6b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with gptj6b. Clearing memory. ---\n",
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--EleutherAI--gpt-j-6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'meta-llama/Llama-2-7b-hf' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da044e5080444ca9b2e852dc492213f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c970cee359064db7a7d2f884dce8336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad116bf6546d4a28aad8937f01fa6761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ae3814caf64fbb91ef4e2f2a4b351e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df664bb317e84352b5d6516bbdf9eab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556687f5bc314b8980b5ab953c3c5447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3308d932ec614b658fe902b43d61ae2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c394d8c6d2314feb99bfe51a27b85e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f48b0c32ef840beb530f3a021996605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b31cd36193a4f1b91120c162170a1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9a7d7172ef43908b9f3e1fb3173901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/llama27b/analogy_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/llama27b/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/llama27b/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task' over 20 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/llama27b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/llama27b/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/llama27b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with llama27b. Clearing memory. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf\n",
      "--- Loading model and tokenizer for 'meta-llama/Llama-2-13b-hf' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922557bf90f47928a720dfebbb67fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba44dd97661b42a79164fd42efaf0ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348aaa26f2f44624a843992045d66aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f8ec59d8ec41f68174e604983962d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362a16584f24433dbae3f9f4ae09df01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbe8231a80c441bb5805192b8600159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba11eda94c64b719b1f86dea79f762e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25af46c438524b20b77aee69a72c6b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e238815fb1414dba6bfa41b8213c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3f47db06124671a200b41092cd5a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b59e21904a74da1a9768cc5dfdcbbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bb12edb05442a5a01a7fd0222a817f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/llama213b/analogy_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/llama213b/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/llama213b/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task' over 20 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_5_Random_MultiRun/llama213b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_5_Random_MultiRun/llama213b/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_5_Random_MultiRun/llama213b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with llama213b. Clearing memory. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12658121/ipykernel_3294046/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf\n",
      "--- Loading model and tokenizer for 'EleutherAI/gpt-neox-20b' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b918b2cbac4489a96e869ce2daa30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59a8b660f264b368dfb9d1cacd901e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae87b4a58a2c40a38f5c2bfcc3209f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d22672025b47b08d62d3e0821f7248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7dddc36139455fb793dd4b60822227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cd1156b2b846838207ca05b70f6ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09db87bad0e463e8899c1e107662f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09beebf5af8347359927c99a4eacff55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 46 files:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e99b8e2a8984821a0428469ae15fc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84233a445d9f40c291969e4ab983e36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f18191fe28242c98439dbdd139af83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00046.safetensors:   0%|          | 0.00/926M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e06ac32af2b459ab9fef7a3628de3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e26d6d305dd4a1487f9b8e6f408c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611c446a97344f1fa733adcaa51f5a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c41a3cf6a943e3839f3d039c571bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41235b5a920f4e289d7a73b006ae4660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cfb570e2da426086cc0321787ffeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ae31b0c0ca4251bbc9aba543318241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916aab5f06f9496fb4423411ee491aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426fb6e490a149c5a3b87d71ad75f5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aea62c7f6d74db885fb5a00ed5251ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e8f77a80f24cb39d1b406cff54b045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdcb8909fec4f01ae631ad4a36110f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf49c47edfd7462abe2d9c1979960bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c4b3a7b4164ebf8c06ad837ce75fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0010dc9452ca413ba7302fb970cbffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011bf05447be4984bfe928286f04e229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8f585d099543c6835d9e5fd9962419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9384a2b7b7f34c54b3e1527e7dfca6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e4be88d57642a497bac71c02e352ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc66ce09586844a9a854c1c6088be100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6a6e72d4724718925c9e97e226e131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e20829eed24423a8b24e30d11d093e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f7aeeb019046bda2a235b024d4b9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ec6b43b12f46aba329d1769066349d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320214441ae245348f94521d70b64682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e656061b361f43c8bbac01188da7ea4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c413bcd8e66940c2b9543e09c4e2cf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9c091f00ba4bb581e9f65bc9f256a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e47de15d34c47a5b296b9fef6e091ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00033-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390e367f37e3439094561e08fd9bfe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d11e0faae24dd98866b3cd9c922602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00034-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51f4d2eb9bd4b929d4bc80adfd0f66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00035-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548e1866f4f14a158a62e6dbd45c440e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00036-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a22483798de4efa83df8b7b4a8f7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00037-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2fa4d2525546d8bf6cd3d13136e8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00038-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b5bf94ce9f454098f4fc18be7e347f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00039-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfc89429f3d45ac8e1141dddd132524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00040-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a6c5d2888a4ee8a56dd73d9f70bdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00041-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574847ddcc7443a186668f14647794fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00042-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0e7d6d303145668fa0061880457220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00043-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c25605d44d4c638ee10b0bd96c9939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00044-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23165cfae9524ab6b925559ebdc5e48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00045-of-00046.safetensors:   0%|          | 0.00/604M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d07e06a21f648ad8aec803e76e314ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00046-of-00046.safetensors:   0%|          | 0.00/620M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73a66eee30e42bd9d76c10fca3397cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
