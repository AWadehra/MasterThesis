{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de925cccebe491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(model: nn.Module, model_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a standardized configuration dictionary for various model architectures.\n",
    "    This provides the correct hook names needed for our specific patching experiment.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"\n",
    "        })\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\",\n",
    "            \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"\n",
    "        })\n",
    "    elif 'llama' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\",\n",
    "            \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"\n",
    "        })\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model architecture for '{model_name}' not recognized. Please add its configuration.\")\n",
    "\n",
    "    return config\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Loads a pretrained Hugging Face model and tokenizer, handling various architectures.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    # --- ADD HUGGING FACE TOKEN HERE FOR GATED MODELS LIKE LLAMA ---\n",
    "    # Replace \"YOUR_HF_TOKEN_HERE\" with your actual token.\n",
    "    # It can be a read-only token for security.\n",
    "    HUGGING_FACE_TOKEN = \"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\"\n",
    "\n",
    "    model_dtype = torch.float16 if any(k in model_name.lower() for k in ['6b', '13b', '20b', '70b']) else torch.float32\n",
    "\n",
    "    if 'llama' in model_name.lower():\n",
    "        if HUGGING_FACE_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "            print(\"Warning: Llama model selected, but no Hugging Face token provided. This may fail.\")\n",
    "            access_token = None\n",
    "        else:\n",
    "            access_token = HUGGING_FACE_TOKEN\n",
    "\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True, token=access_token).to(device)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    model_config = get_model_config(model, model_name)\n",
    "\n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8c5e770643139b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\n",
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt2' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neo-125m' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_heads,\n",
    "                      \"n_layers\":model.config.num_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neo.layers.{layer}.attention.dense' for layer in range(model.config.num_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neo.layers.{layer}' for layer in range(model.config.num_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower() or 'pythia' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neox.layers.{layer}.attention.dense' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neox.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                token=access_token\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower() or '8b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else: #half precision for bigger llama models\n",
    "                #This becomes only for the 13B model then. Okay then. What else?\n",
    "                model_dtype = torch.float16\n",
    "\n",
    "            # If transformers version is < 4.31 use LlamaLoaders\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            # model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "            # If transformers version is >= 4.31, use AutoLoaders\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, token=access_token).to(device)\n",
    "\n",
    "        # --- THIS IS THE FIX ---\n",
    "        if tokenizer.pad_token is None:\n",
    "            print(\"Llama tokenizer does not have a pad token. Setting pad_token = eos_token.\")\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = model.config.eos_token_id\n",
    "        \n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\":model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config._name_or_path,\n",
    "                      \"attn_hook_names\":[f'model.layers.{layer}.self_attn.o_proj' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'model.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":True}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e81171-06f0-4c34-8a1f-ef91dbdff202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer. This version is corrected to\n",
    "    handle all model architectures and create the correct config dictionary.\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    HUGGING_FACE_TOKEN = \"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\" # User's provided token\n",
    "    kwargs = {'low_cpu_mem_usage': True}\n",
    "    \n",
    "    # --- Model Loading Logic ---\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        print(\"Using float16 revision for gpt-j-6B to ensure PyTorch-only workflow.\")\n",
    "        kwargs['revision'] = 'float16'\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    elif 'llama' in model_name.lower():\n",
    "        access_token = HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != \"YOUR_HF_TOKEN_HERE\" else None\n",
    "        if not access_token: print(\"Warning: Llama model selected, but no Hugging Face token provided.\")\n",
    "        kwargs['token'] = access_token\n",
    "        if '70b' in model_name.lower():\n",
    "            bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "            kwargs['quantization_config'] = bnb_config\n",
    "            kwargs['trust_remote_code'] = True\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "        else:\n",
    "            kwargs['torch_dtype'] = torch.float16 if any(k in model_name.lower() for k in ['13b']) else torch.float32\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16 if '20b' in model_name.lower() else torch.float32\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # --- THIS IS THE FIX: Create the config dict with the correct keys ---\n",
    "    model_config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"})\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\", \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"})\n",
    "    elif 'llama' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\", \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"})\n",
    "    else: raise NotImplementedError(\"Model architecture not recognized.\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca8a232f4254d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Defines the clean and corrupted prompts and answers for each task.\n",
    "    This uses the single-prompt (\"toy data\") setup as specified.\n",
    "    \"\"\"\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs No Rule)\",\n",
    "            \"clean_prompt\": \"ABCD:ABCE::JKLM:JKL\",\n",
    "            \"clean_correct_answer\": \"N\",\n",
    "            \"clean_incorrect_answer\": \"M\",\n",
    "            \"corrupted_prompt\": \"ABCD:ABCD::JKLM:JKL\",\n",
    "            \"corrupted_correct_answer\": \"M\",\n",
    "            \"corrupted_incorrect_answer\": \"N\",\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"clean_prompt\": \"January:February::Wednesday:\",\n",
    "            \"clean_correct_answer\": \"Thursday\",\n",
    "            \"clean_incorrect_answer\": \"Wednesday\",\n",
    "            \"corrupted_prompt\": \"January:January::Wednesday:\",\n",
    "            \"corrupted_correct_answer\": \"Wednesday\",\n",
    "            \"corrupted_incorrect_answer\": \"Thursday\",\n",
    "        }\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5ed801d717061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "def caching_hook_factory(hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        activation_cache[hook_name] = tensor_to_cache.detach()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in activation_cache: raise ValueError(f\"Activation for {hook_name} not found!\")\n",
    "        cached_activation = activation_cache[hook_name]\n",
    "        patched_output = output.clone()\n",
    "        min_seq_len = min(patched_output.shape[-2], cached_activation.shape[-2])\n",
    "        if head_index is not None:\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            if patched_output.ndim == 3: patched_output[:, :min_seq_len, start:end] = cached_activation[:, :min_seq_len, start:end]\n",
    "            elif patched_output.ndim == 2: patched_output[:min_seq_len, start:end] = cached_activation[:min_seq_len, start:end]\n",
    "        else:\n",
    "            if patched_output.ndim == 3: patched_output[:, :min_seq_len, :] = cached_activation[:, :min_seq_len, :]\n",
    "            elif patched_output.ndim == 2: patched_output[:min_seq_len, :] = cached_activation[:min_seq_len, :]\n",
    "        return patched_output\n",
    "    return hook\n",
    "\n",
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'): model = getattr(model, part)\n",
    "    return model\n",
    "\n",
    "def run_with_hooks(model: nn.Module, tokenizer: AutoTokenizer, prompt: str, hook_fns: Dict[str, Callable]) -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module = get_module_by_name(model, name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(model.device)\n",
    "        with torch.no_grad(): outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles: handle.remove()\n",
    "\n",
    "def calculate_logit_diff(logits: torch.Tensor, tokenizer: AutoTokenizer, correct_answer: str, incorrect_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the logit difference.\n",
    "    FIX: Uses `add_special_tokens=False` to prevent the tokenizer from adding\n",
    "    a Beginning-Of-Sentence token, which would make the logit difference always zero.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the ID for the token without any special additions (like BOS)\n",
    "        correct_id = tokenizer.encode(correct_answer, add_special_tokens=False)[0]\n",
    "        incorrect_id = tokenizer.encode(incorrect_answer, add_special_tokens=False)[0]\n",
    "\n",
    "        return (logits[correct_id] - logits[incorrect_id]).item()\n",
    "\n",
    "    except IndexError:\n",
    "        # This can happen if the tokenizer returns an empty list for a given string\n",
    "        print(f\"Warning: Tokenizer failed to encode '{correct_answer}' or '{incorrect_answer}'.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in calculate_logit_diff: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32dcfdfe06c5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_patching_experiment(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, source_prompt: str, dest_prompt: str, dest_correct_answer: str, dest_incorrect_answer: str, layer: int, component_type: str, head_index: int = None) -> float:\n",
    "    global activation_cache\n",
    "    activation_cache = {}\n",
    "    hook_template = model_config['mlp_hook_name_template'] if component_type == 'mlp' else model_config['attn_hook_name_template']\n",
    "    hook_name = hook_template.format(layer)\n",
    "    run_with_hooks(model, tokenizer, source_prompt, {hook_name: caching_hook_factory(hook_name)})\n",
    "    d_head = model_config[\"d_model\"] // model_config[\"n_heads\"] if component_type == 'attn_head' else None\n",
    "    patching_hooks = {hook_name: patching_hook_factory(hook_name, head_index, d_head)}\n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, task_data: Dict, patch_type: str) -> pd.DataFrame:\n",
    "    n_layers, n_heads = model_config[\"n_layers\"], model_config[\"n_heads\"]\n",
    "    if patch_type == 'noising':\n",
    "        source_prompt, dest_prompt = task_data['corrupted_prompt'], task_data['clean_prompt']\n",
    "        dest_correct, dest_incorrect = task_data['clean_correct_answer'], task_data['clean_incorrect_answer']\n",
    "    else: # denoising\n",
    "        source_prompt, dest_prompt = task_data['clean_prompt'], task_data['corrupted_prompt']\n",
    "        dest_correct, dest_incorrect = task_data['corrupted_correct_answer'], task_data['corrupted_incorrect_answer']\n",
    "\n",
    "    clean_logits = run_with_hooks(model, tokenizer, task_data['clean_prompt'], {})\n",
    "    clean_baseline = calculate_logit_diff(clean_logits, tokenizer, task_data['clean_correct_answer'], task_data['clean_incorrect_answer'])\n",
    "    corrupted_logits = run_with_hooks(model, tokenizer, task_data['corrupted_prompt'], {})\n",
    "    corrupted_baseline = calculate_logit_diff(corrupted_logits, tokenizer, task_data['corrupted_correct_answer'], task_data['corrupted_incorrect_answer'])\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}'\")\n",
    "\n",
    "    results = []\n",
    "    for component_type, head_range in [('attn_head', range(n_heads)), ('mlp', [-1])]:\n",
    "        print(f\"  - Patching {component_type}s...\")\n",
    "        for layer in range(n_layers):\n",
    "            for head_index in head_range:\n",
    "                patched_logit_diff = perform_patching_experiment(model, tokenizer, model_config, source_prompt, dest_prompt, dest_correct, dest_incorrect, layer, component_type, head_index if component_type == 'attn_head' else None)\n",
    "                if patch_type == 'noising':\n",
    "                    effect = patched_logit_diff - clean_baseline\n",
    "                else:\n",
    "                    effect = patched_logit_diff - corrupted_baseline\n",
    "                results.append({'layer': layer, 'head': head_index, 'type': component_type, 'effect': effect})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3467394fb471dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    if component_type == 'attn_head':\n",
    "        if df[df['type'] == 'attn_head'].empty: return\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='head', columns='layer', values='effect')\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        max_abs_val = pivot_df.abs().max().max() if not pivot_df.empty else 1.0\n",
    "        im = ax.imshow(pivot_df, cmap='coolwarm', vmin=-max_abs_val, vmax=max_abs_val, aspect='auto')\n",
    "        cbar = ax.figure.colorbar(im, ax=ax); cbar.ax.set_ylabel(\"Effect on Logit Difference\", rotation=-90, va=\"bottom\")\n",
    "        ax.set_xticks(np.arange(pivot_df.shape[1])); ax.set_yticks(np.arange(pivot_df.shape[0]))\n",
    "        ax.set_xticklabels(pivot_df.columns); ax.set_yticklabels(pivot_df.index)\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Head Index\")\n",
    "    elif component_type == 'mlp':\n",
    "        if df[df['type'] == 'mlp'].empty: return\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(mlp_df['layer'], mlp_df['effect'], color='skyblue')\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Average Effect on Logit Difference\")\n",
    "        ax.grid(axis='y', linestyle='--'); ax.set_xticks(mlp_df['layer'])\n",
    "    else: raise ValueError(\"Invalid component type\")\n",
    "    ax.set_title(title); fig.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf'); print(f\"Saved plot to: {output_path}\")\n",
    "    plt.show(); plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bcdfdc6df219ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the activation patching experiment across multiple models.\n",
    "    \"\"\"\n",
    "    # This model dictionary is taken directly from the user's script\n",
    "    models_to_test = {\n",
    "        #'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        'gpt2': 'gpt2', # Added gpt2 for a quick baseline\n",
    "        'gptj6b': 'EleutherAI/gpt-j-6B',\n",
    "        'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "        'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "        'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "        'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    main_output_dir = \"Results_3\"\n",
    "    os.makedirs(main_output_dir, exist_ok=True)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    for model_short_name, model_hf_name in models_to_test.items():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_hf_name)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- Could not load model {model_hf_name}. Skipping. Error: {e} ---\\n\")\n",
    "            \n",
    "            from transformers import file_utils\n",
    "            import shutil\n",
    "            import re\n",
    "    \n",
    "            # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "            model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "            cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "    \n",
    "            # 2. Delete only this model's folder\n",
    "            if os.path.exists(cache_path):\n",
    "                print(f\"Deleting model cache: {cache_path}\")\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "                \n",
    "            continue\n",
    "\n",
    "        # Create model-specific subdirectory inside the main \"Results\" folder\n",
    "        model_results_dir = os.path.join(main_output_dir, model_short_name)\n",
    "        os.makedirs(model_results_dir, exist_ok=True)\n",
    "\n",
    "        for task_name, task_data in datasets.items():\n",
    "            for patch_type in ['noising']:\n",
    "                result_key = f\"{task_name}_{patch_type}\"\n",
    "                df = run_exploratory_sweep(model, tokenizer, model_config, task_data, patch_type)\n",
    "\n",
    "                # Save CSV and Plots inside the model-specific subfolder\n",
    "                csv_path = os.path.join(model_results_dir, f\"{result_key}_results.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved data to: {csv_path}\")\n",
    "\n",
    "                plot_path_attn = os.path.join(model_results_dir, f\"{result_key}_attn_heads.pdf\")\n",
    "                plot_path_mlp = os.path.join(model_results_dir, f\"{result_key}_mlp_layers.pdf\")\n",
    "                title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                plot_results(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "                plot_results(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "        print(f\"--- Finished with {model_short_name}. Clearing memory. ---\")\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # --- NEW: Delete ONLY this model's cache ---\n",
    "        from transformers import file_utils\n",
    "        import shutil\n",
    "        import re\n",
    "\n",
    "        # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "        model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "        cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "\n",
    "        # 2. Delete only this model's folder\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Deleting model cache: {cache_path}\")\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4650f51bc272a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'gpt2' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dce94d4ec64771867ad19327f6b090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13c5ce8347145028d966b36c2277c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b056ad59c858462ca5cf48139d567604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba52a28d81f4da790cae5fc80617d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb4c5f387b046cb98899d25dfc956ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf47233f0a14d8b93080fe6f803c3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126525dc74cd40fea447f3aff34e11fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/gpt2/analogy_noising_results.csv\n",
      "Saved plot to: Results_3/gpt2/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/gpt2/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task'\n",
      "  - Patching attn_heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching mlps...\n",
      "Saved data to: Results_3/gpt2/sequencing_noising_results.csv\n",
      "Saved plot to: Results_3/gpt2/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/gpt2/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with gpt2. Clearing memory. ---\n",
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--gpt2\n",
      "--- Loading model and tokenizer for 'EleutherAI/gpt-j-6B' ---\n",
      "Using float16 revision for gpt-j-6B to ensure PyTorch-only workflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64138f761da4e2fb38032447bd5d873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca344b54a9944100908cb23b49ddd918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fabbceb14e14e308770d7fb705ba84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792489ad721441e992252de40a67701a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9356448f030495caeb3f4257563921a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c89ff95d3ca4cefa9badb137098d529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371db701ba98401aa663de69a269c4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/836 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cfafefeec34cb79b52879b85e2c9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/gptj6b/analogy_noising_results.csv\n",
      "Saved plot to: Results_3/gptj6b/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/gptj6b/analogy_noising_mlp_layers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/gptj6b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_3/gptj6b/sequencing_noising_attn_heads.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: Results_3/gptj6b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with gptj6b. Clearing memory. ---\n",
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--EleutherAI--gpt-j-6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'meta-llama/Llama-2-7b-hf' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9e523936b5477fb78a03862b7cccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1d5307197b46f0b4ab1337b70920d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47f6921027348fe85fec2a258b7ac66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fd4d35fe544defa58171753e571c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee819d19efdb41ee9220f1462bb89b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e28810bcdc5461bb1809c78101588c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7f4c99e60d4ca4b01b37cade91d506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e27a5aab93841628feafce06b1b283b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc11628f90514e4b8c045675bd64c7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d55b96e6f4d86929623975ca228b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddb5859b2bb4837bb20121f64846ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/llama27b/analogy_noising_results.csv\n",
      "Saved plot to: Results_3/llama27b/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/llama27b/analogy_noising_mlp_layers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/llama27b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_3/llama27b/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/llama27b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with llama27b. Clearing memory. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf\n",
      "--- Loading model and tokenizer for 'meta-llama/Llama-2-13b-hf' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f9f9b387ed448184649ff2aa56b55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e876da07a247cbbf7cbcb9369f661a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba193e3284b495ba770618224c20301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d4e86aa6fe45d79a184fd2ad870a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52cf2bd87bf4a9d9c4a43874a0b34c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314f122541654217bf45bd8a01efc00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895fa96c320849df84bb46766d9fb020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa432993e25470aa753eca1f61a44bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c93d749385f43f2aba7cc9c571e1b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5802c7268f0d4bb1b34544ce8f2667a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7465fef4859f4b80a1bd7989f4a95044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4062f50106b643099c1066fc90ca7e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/llama213b/analogy_noising_results.csv\n",
      "Saved plot to: Results_3/llama213b/analogy_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/llama213b/analogy_noising_mlp_layers.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/llama213b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_3/llama213b/sequencing_noising_attn_heads.pdf\n",
      "Saved plot to: Results_3/llama213b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with llama213b. Clearing memory. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n",
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf\n",
      "--- Loading model and tokenizer for 'EleutherAI/gpt-neox-20b' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11fe61531eb4a0e8c48bea588458a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f0af01e4c141988c5c71337774af41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8646a6be371422a981e8769c38bd636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/457k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b232487e26784fe8962730fd71acabea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f218624b4d0400698bdfd3d3e445609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e293b1c73fc649e088eea35bfc5cfde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f56d1eb593840eb96358f76ff685f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/60.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfa7cd5de2c44019491280901351e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 46 files:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756d4c907d744d1e9ee14aa868b9b660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61d4caf03a24ddc95092238513c2deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7b93642090499c9e4dbae1f1d13c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ffed3a12b340478b6e9cc35209e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f0e55a5a7f430ba2b5f5ee69620fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04693f835cc4723940423975efcfcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b51d78228b4d91a4ef5431c1c588f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fef7e5d294431398df9e56983d775c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00046.safetensors:   0%|          | 0.00/926M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f39f082cafa43ea944d9be01b2f5aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69a03bb9fc74142976c69fd0133f855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aa5f06a9f440daa983f8d9cb0ab15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7356255198ef4c49b12b7e569b63c829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999c5799bff540eb9382dba24f162feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5190a2dc8442401a93e242286c2c1d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8bc96b447a4ec1ae1bb26c2126ca40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e92a6bd0364e83acbf3e7abf534344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c96fa3e52e494e9daadee2c61e207a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cc5a41723f41a9b0d4197eafbd4b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee37c6fcfd3404bbd2c68df48e1beaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d9213881b42cc8495cb9b078945cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5707a608678b4a5ba07c9e2e84b8ee98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd93a1ee6a640258b07f46a7aba48a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d890f3abb3d48129b2155f958ffa288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc29c13dbdf4c348daa2561ec549ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e380e47362f84ea0816c722268f174e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7a31737f7a42ff89681b8ffafac0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6c61ae437043e1a5c61041596f6ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ab842a89224456bb1ccb7cd7587543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da93ce6a32c42d0b3091a35f5d07e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1a6bd7aa004fe4b54673f426c5ba72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e682d09cc64a3bbf78a4b1960529c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec1284c21ef4de88c75d7c61b18a84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523b33346c5549c1956cce43e0e0cf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00033-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09870f3af589445bac6a9a0f48c3fe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00034-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a52bb3ac8b4b198567c92ee648655d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00035-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606451ba6b82482295f090a754683338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00036-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4443c43d1194782aa0464f8196b32a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00037-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8092c6bd5ea44fc83216223dc2bc29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00038-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0df71c4f24ad18350e4f9fb374a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00039-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50e271f1fe241bdb74c5494593903fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00040-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93610532afc749a589892cf80af66ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00041-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765d09759c7d441fae9668a3f16d275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00042-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e223649baa6a4f5c8c49cd4d8d5059a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00043-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fc4442666a4422926b76220ebcb9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00044-of-00046.safetensors:   0%|          | 0.00/910M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1d99e5dfda4317ab4d74266304e56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00045-of-00046.safetensors:   0%|          | 0.00/604M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c302775a0441769099c65f0c894dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00046-of-00046.safetensors:   0%|          | 0.00/620M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d81c01cfe1c4869bd408943d32e678b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)'\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_3/gptneox20b/analogy_noising_results.csv\n",
      "Saved plot to: Results_3/gptneox20b/analogy_noising_attn_heads.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: Results_3/gptneox20b/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task'\n",
      "  - Patching attn_heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching mlps...\n",
      "Saved data to: Results_3/gptneox20b/sequencing_noising_results.csv\n",
      "Saved plot to: Results_3/gptneox20b/sequencing_noising_attn_heads.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: Results_3/gptneox20b/sequencing_noising_mlp_layers.pdf\n",
      "--- Finished with gptneox20b. Clearing memory. ---\n",
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--EleutherAI--gpt-neox-20b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12574629/ipykernel_1053901/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'meta-llama/Llama-2-70b-hf' ---\n",
      "\n",
      "--- Could not load model meta-llama/Llama-2-70b-hf. Skipping. Error: name 'BitsAndBytesConfig' is not defined ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
