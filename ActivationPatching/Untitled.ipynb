{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede8fe67-fc0a-4c8f-af53-28a65f7c1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Callable, List\n",
    "import json\n",
    "import random\n",
    "#import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35a2214-a796-413f-b74c-5930ec5adfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 1: ADD DETERMINISM FOUNDATION =====\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "# ===== END CHANGE 1 ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c4dea2-5ef5-4a2d-a98e-b0f3d5155e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(model: nn.Module, model_name: str) -> Dict:\n",
    "    config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"\n",
    "        })\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\",\n",
    "            \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"\n",
    "        })\n",
    "    elif 'llama' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\",\n",
    "            \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"\n",
    "        })\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model architecture for '{model_name}' not recognized. Please add its configuration.\")\n",
    "\n",
    "    return config\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str, device: str = 'cuda'):\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    HUGGING_FACE_TOKEN = \"hf_findNewOne\"\n",
    "    model_dtype = torch.float16 if any(k in model_name.lower() for k in ['6b', '13b', '20b', '70b']) else torch.float32\n",
    "\n",
    "    if 'llama' in model_name.lower():\n",
    "        if HUGGING_FACE_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "            print(\"Warning: Llama model selected, but no Hugging Face token provided. This may fail.\")\n",
    "            access_token = None\n",
    "        else:\n",
    "            access_token = HUGGING_FACE_TOKEN\n",
    "\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True, token=access_token).to(device)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True).to(device)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # ===== CHANGE 6: LAYERNORM STABILIZATION =====\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.LayerNorm):\n",
    "            module.eps = 1e-3  # Increased from 1e-5/1e-6\n",
    "    # ===== END CHANGE 6 =====\n",
    "    \n",
    "    model_config = get_model_config(model, model_name)\n",
    "\n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5d7671-4bf8-484f-baa8-ba12a4dcaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    assert model_name is not None\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    HUGGING_FACE_TOKEN = \"hf_findNewOne\"\n",
    "    kwargs = {'low_cpu_mem_usage': True}\n",
    "    \n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        print(\"Using float16 revision for gpt-j-6B to ensure PyTorch-only workflow.\")\n",
    "        kwargs['revision'] = 'float16'\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    elif 'llama' in model_name.lower():\n",
    "        access_token = HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != \"YOUR_HF_TOKEN_HERE\" else None\n",
    "        if not access_token: print(\"Warning: Llama model selected, but no Hugging Face token provided.\")\n",
    "        kwargs['token'] = access_token\n",
    "        if '70b' in model_name.lower():\n",
    "            bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', \n",
    "                                           bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "            kwargs['quantization_config'] = bnb_config\n",
    "            kwargs['trust_remote_code'] = True\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "        else:\n",
    "            kwargs['torch_dtype'] = torch.float16 if any(k in model_name.lower() for k in ['13b']) else torch.float32\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16 if '20b' in model_name.lower() else torch.float32\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # ===== CHANGE 6: LAYERNORM STABILIZATION =====\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.LayerNorm):\n",
    "            module.eps = 1e-3  # Increased from 1e-5/1e-6\n",
    "    # ===== END CHANGE 6 =====\n",
    "    \n",
    "    model_config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"})\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\", \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"})\n",
    "    elif 'llama' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\", \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"})\n",
    "    else: raise NotImplementedError(\"Model architecture not recognized.\")\n",
    "\n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0ad084-3b30-4484-afed-be26d3d90f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    # ===== CHANGE 2: SEED PROMPT SAMPLING =====\n",
    "    random.seed(SEED)\n",
    "    # ===== END CHANGE 2 =====\n",
    "    \n",
    "    with open('../BaselineAccuracy/dataset_files/abstractive/succ_letterstring_basic.json', 'r') as f:\n",
    "        analogy_data = json.load(f)\n",
    "\n",
    "    analogy_prompts = []\n",
    "    for _ in range(20):\n",
    "        # ===== CHANGE 2: USE CHOICE INSTEAD OF SAMPLE =====\n",
    "        example_pair = random.choice(analogy_data)\n",
    "        target_pair = random.choice(analogy_data)\n",
    "        # ===== END CHANGE 2 =====\n",
    "\n",
    "        example_in = example_pair['input'].strip('[]').replace(' ', ':')\n",
    "        example_out = example_pair['output'].strip('[]').replace(' ', ':')\n",
    "        target_in_full = target_pair['input'].strip('[]').replace(' ', ':')\n",
    "        target_out_full = target_pair['output'].strip('[]').replace(' ', ':')\n",
    "\n",
    "        target_in_parts = target_in_full.split(':')\n",
    "        target_prefix = \":\".join(target_in_parts[:3]) + \":\"\n",
    "        correct_answer = \" \" + target_out_full.split(':')[-1]\n",
    "        incorrect_answer = \" \" + target_in_full.split(':')[-1]\n",
    "\n",
    "        analogy_prompts.append({\n",
    "            \"clean_prompt\": f\"{example_in}:{example_out}::{target_in_full}:{target_prefix}\",\n",
    "            \"clean_correct_answer\": correct_answer,\n",
    "            \"clean_incorrect_answer\": incorrect_answer,\n",
    "            \"corrupted_prompt\": f\"{example_in}:{example_in}::{target_in_full}:{target_prefix}\",\n",
    "            \"corrupted_correct_answer\": incorrect_answer,\n",
    "            \"corrupted_incorrect_answer\": correct_answer,\n",
    "        })\n",
    "\n",
    "    with open('../BaselineAccuracy/dataset_files/abstractive/next_item.json', 'r') as f:\n",
    "        sequencing_data = json.load(f)\n",
    "\n",
    "    sequencing_prompts = []\n",
    "    for _ in range(20):\n",
    "        # ===== CHANGE 2: USE CHOICE INSTEAD OF SAMPLE =====\n",
    "        example_pair = random.choice(sequencing_data)\n",
    "        target_pair = random.choice(sequencing_data)\n",
    "        # ===== END CHANGE 2 =====\n",
    "\n",
    "        example_in, example_out = \" \" + example_pair['input'], \" \" + example_pair['output']\n",
    "        target_in, target_out = \" \" + target_pair['input'], \" \" + target_pair['output']\n",
    "\n",
    "        sequencing_prompts.append({\n",
    "            \"clean_prompt\": f\"{example_in}:{example_out}::{target_in}:\",\n",
    "            \"clean_correct_answer\": target_out,\n",
    "            \"clean_incorrect_answer\": target_in,\n",
    "            \"corrupted_prompt\": f\"{example_in}:{example_in}::{target_in}:\",\n",
    "            \"corrupted_correct_answer\": target_in,\n",
    "            \"corrupted_incorrect_answer\": target_out,\n",
    "        })\n",
    "    \n",
    "    # ===== CHANGE 8: POSITION CONSISTENCY =====\n",
    "    max_length = max(\n",
    "        max(len(p['clean_prompt']) for p in analogy_prompts),\n",
    "        max(len(p['clean_prompt']) for p in sequencing_prompts)\n",
    "    ) + 5\n",
    "    \n",
    "    for dataset in [analogy_prompts, sequencing_prompts]:\n",
    "        for p in dataset:\n",
    "            p['clean_prompt'] = p['clean_prompt'].ljust(max_length)\n",
    "            p['corrupted_prompt'] = p['corrupted_prompt'].ljust(max_length)\n",
    "    # ===== END CHANGE 8 =====\n",
    "\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs No Rule)\",\n",
    "            \"prompts\": analogy_prompts\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"prompts\": sequencing_prompts\n",
    "        }\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c33a2fa-d7c6-471b-bf58-e5eb42f46ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 4: LOCAL CACHE ISOLATION =====\n",
    "def caching_hook_factory(cache: dict, hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        cache[hook_name] = tensor_to_cache.detach().clone()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(cache: dict, hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in cache:\n",
    "            raise ValueError(f\"Activation for {hook_name} not found!\")\n",
    "        cached_activation = cache[hook_name]\n",
    "        patched_output = output.clone()\n",
    "        min_seq_len = min(patched_output.shape[-2], cached_activation.shape[-2])\n",
    "        if head_index is not None:\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            if patched_output.ndim == 3: \n",
    "                patched_output[:, :min_seq_len, start:end] = cached_activation[:, :min_seq_len, start:end]\n",
    "            elif patched_output.ndim == 2: \n",
    "                patched_output[:min_seq_len, start:end] = cached_activation[:min_seq_len, start:end]\n",
    "        else:\n",
    "            if patched_output.ndim == 3: \n",
    "                patched_output[:, :min_seq_len, :] = cached_activation[:, :min_seq_len, :]\n",
    "            elif patched_output.ndim == 2: \n",
    "                patched_output[:min_seq_len, :] = cached_activation[:min_seq_len, :]\n",
    "        return patched_output\n",
    "    return hook\n",
    "# ===== END CHANGE 4 =====\n",
    "\n",
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'): model = getattr(model, part)\n",
    "    return model\n",
    "\n",
    "def run_with_hooks(model: nn.Module, tokenizer: AutoTokenizer, prompt: str, hook_fns: Dict[str, Callable]) -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module = get_module_by_name(model, name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "        \n",
    "        # ===== CHANGE 7: ATTENTION MASK ENFORCEMENT =====\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move ALL to device\n",
    "        # ===== END CHANGE 7 =====\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles: handle.remove()\n",
    "\n",
    "def calculate_logit_diff(logits: torch.Tensor, tokenizer: AutoTokenizer, correct_answer: str, incorrect_answer: str) -> float:\n",
    "    try:\n",
    "        # ===== CHANGE 3: TOKENIZATION SAFETY =====\n",
    "        correct_tokens = tokenizer.encode(correct_answer, add_special_tokens=False)\n",
    "        incorrect_tokens = tokenizer.encode(incorrect_answer, add_special_tokens=False)\n",
    "        \n",
    "        if not correct_tokens or not incorrect_tokens:\n",
    "            return 0.0\n",
    "            \n",
    "        correct_id = correct_tokens[0]\n",
    "        incorrect_id = incorrect_tokens[0]\n",
    "        # ===== END CHANGE 3 =====\n",
    "\n",
    "        return (logits[correct_id] - logits[incorrect_id]).item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_logit_diff: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def perform_patching_experiment(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, \n",
    "                               source_prompt: str, dest_prompt: str, dest_correct_answer: str, \n",
    "                               dest_incorrect_answer: str, layer: int, component_type: str, \n",
    "                               head_index: int = None) -> float:\n",
    "    # ===== CHANGE 4: LOCAL CACHE ISOLATION =====\n",
    "    local_cache = {}\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    hook_template = model_config['mlp_hook_name_template'] if component_type == 'mlp' else model_config['attn_hook_name_template']\n",
    "    hook_name = hook_template.format(layer)\n",
    "    \n",
    "    # ===== CHANGE 4: USE LOCAL CACHE =====\n",
    "    run_with_hooks(model, tokenizer, source_prompt, \n",
    "                  {hook_name: caching_hook_factory(local_cache, hook_name)})\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    d_head = model_config[\"d_model\"] // model_config[\"n_heads\"] if component_type == 'attn_head' else None\n",
    "    \n",
    "    # ===== CHANGE 4: USE LOCAL CACHE =====\n",
    "    patching_hooks = {hook_name: patching_hook_factory(local_cache, hook_name, head_index, d_head)}\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, task_data: Dict, patch_type: str) -> pd.DataFrame:\n",
    "    n_layers, n_heads = model_config[\"n_layers\"], model_config[\"n_heads\"]\n",
    "    prompt_dataset = task_data['prompts']\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}' over {len(prompt_dataset)} prompts...\")\n",
    "\n",
    "    clean_baselines, corrupted_baselines = [], []\n",
    "    for prompt_set in prompt_dataset:\n",
    "        clean_logits = run_with_hooks(model, tokenizer, prompt_set['clean_prompt'], {})\n",
    "        clean_baselines.append(calculate_logit_diff(clean_logits, tokenizer, prompt_set['clean_correct_answer'], prompt_set['clean_incorrect_answer']))\n",
    "        corrupted_logits = run_with_hooks(model, tokenizer, prompt_set['corrupted_prompt'], {})\n",
    "        corrupted_baselines.append(calculate_logit_diff(corrupted_logits, tokenizer, prompt_set['corrupted_correct_answer'], prompt_set['corrupted_incorrect_answer']))\n",
    "\n",
    "    results = []\n",
    "    for component_type, head_range in [('attn_head', range(n_heads)), ('mlp', [-1])]:\n",
    "        print(f\"  - Patching {component_type}s...\")\n",
    "        for layer in range(n_layers):\n",
    "            for head_index in head_range:\n",
    "                effects_for_this_component = []\n",
    "                for i, prompt_set in enumerate(prompt_dataset):\n",
    "                    if patch_type == 'noising':\n",
    "                        source_prompt, dest_prompt = prompt_set['corrupted_prompt'], prompt_set['clean_prompt']\n",
    "                        dest_correct, dest_incorrect = prompt_set['clean_correct_answer'], prompt_set['clean_incorrect_answer']\n",
    "                        baseline_to_compare = clean_baselines[i]\n",
    "                    else: # denoising\n",
    "                        source_prompt, dest_prompt = prompt_set['clean_prompt'], prompt_set['corrupted_prompt']\n",
    "                        dest_correct, dest_incorrect = prompt_set['corrupted_correct_answer'], prompt_set['corrupted_incorrect_answer']\n",
    "                        baseline_to_compare = corrupted_baselines[i]\n",
    "\n",
    "                    patched_logit_diff = perform_patching_experiment(\n",
    "                        model, tokenizer, model_config, source_prompt, dest_prompt, \n",
    "                        dest_correct, dest_incorrect, layer, component_type, \n",
    "                        head_index if component_type == 'attn_head' else None\n",
    "                    )\n",
    "                    effect = patched_logit_diff - baseline_to_compare\n",
    "                    \n",
    "                    # ===== CHANGE 5: OUTLIER FILTER =====\n",
    "                    if abs(effect) > 10:  # Filter extreme outliers\n",
    "                        effect = 0\n",
    "                    # ===== END CHANGE 5 =====\n",
    "                    \n",
    "                    effects_for_this_component.append(effect)\n",
    "\n",
    "                # ===== CHANGE 5: ROBUST AGGREGATION =====\n",
    "                average_effect = np.median(effects_for_this_component)  # Use median instead of mean\n",
    "                # ===== END CHANGE 5 =====\n",
    "                \n",
    "                results.append({'layer': layer, 'head': head_index, 'type': component_type, 'effect': average_effect})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e4e9db-1757-4dd2-9333-594696d0085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    if component_type == 'attn_head':\n",
    "        if df[df['type'] == 'attn_head'].empty: return\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='head', columns='layer', values='effect')\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        max_abs_val = pivot_df.abs().max().max() if not pivot_df.empty else 1.0\n",
    "        im = ax.imshow(pivot_df, cmap='coolwarm', vmin=-max_abs_val, vmax=max_abs_val, aspect='auto')\n",
    "        cbar = ax.figure.colorbar(im, ax=ax); cbar.ax.set_ylabel(\"Effect on Logit Difference\", rotation=-90, va=\"bottom\")\n",
    "        ax.set_xticks(np.arange(pivot_df.shape[1])); ax.set_yticks(np.arange(pivot_df.shape[0]))\n",
    "        ax.set_xticklabels(pivot_df.columns); ax.set_yticklabels(pivot_df.index)\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Head Index\")\n",
    "    elif component_type == 'mlp':\n",
    "        if df[df['type'] == 'mlp'].empty: return\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(mlp_df['layer'], mlp_df['effect'], color='skyblue')\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Average Effect on Logit Difference\")\n",
    "        ax.grid(axis='y', linestyle='--'); ax.set_xticks(mlp_df['layer'])\n",
    "    else: raise ValueError(\"Invalid component type\")\n",
    "    ax.set_title(title); fig.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf'); print(f\"Saved plot to: {output_path}\")\n",
    "    plt.show(); plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575bd58f-66c5-4a4e-85e4-a40834255fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'gpt2' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a654a9e34a4c44a5733f3aa8d8bb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b13a1b2cad4c83af0eccab0c0ede7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7065cf2d7a654c5ba81ef03ff9346d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76ec2ee8448403d8e1d180032f1d62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73e35681df4459a8504595eb6141c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3820dc826f6e441783add8184cf97435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658db40d87a64d73a6dc9aa348c5e101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 48\u001B[0m\n\u001B[1;32m     45\u001B[0m         torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 48\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 30\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m patch_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnoising\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m     29\u001B[0m     result_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtask_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpatch_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 30\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mrun_exploratory_sweep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatch_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     csv_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_results_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_results.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m     df\u001B[38;5;241m.\u001B[39mto_csv(csv_path, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[8], line 124\u001B[0m, in \u001B[0;36mrun_exploratory_sweep\u001B[0;34m(model, tokenizer, model_config, task_data, patch_type)\u001B[0m\n\u001B[1;32m    121\u001B[0m     dest_correct, dest_incorrect \u001B[38;5;241m=\u001B[39m prompt_set[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcorrupted_correct_answer\u001B[39m\u001B[38;5;124m'\u001B[39m], prompt_set[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcorrupted_incorrect_answer\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    122\u001B[0m     baseline_to_compare \u001B[38;5;241m=\u001B[39m corrupted_baselines[i]\n\u001B[0;32m--> 124\u001B[0m patched_logit_diff \u001B[38;5;241m=\u001B[39m \u001B[43mperform_patching_experiment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdest_correct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest_incorrect\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomponent_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_index\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcomponent_type\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattn_head\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    128\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m effect \u001B[38;5;241m=\u001B[39m patched_logit_diff \u001B[38;5;241m-\u001B[39m baseline_to_compare\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m# ===== CHANGE 5: OUTLIER FILTER =====\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[8], line 92\u001B[0m, in \u001B[0;36mperform_patching_experiment\u001B[0;34m(model, tokenizer, model_config, source_prompt, dest_prompt, dest_correct_answer, dest_incorrect_answer, layer, component_type, head_index)\u001B[0m\n\u001B[1;32m     89\u001B[0m patching_hooks \u001B[38;5;241m=\u001B[39m {hook_name: patching_hook_factory(local_cache, hook_name, head_index, d_head)}\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m# ===== END CHANGE 4 =====\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m patched_logits \u001B[38;5;241m=\u001B[39m \u001B[43mrun_with_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdest_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatching_hooks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
      "Cell \u001B[0;32mIn[8], line 47\u001B[0m, in \u001B[0;36mrun_with_hooks\u001B[0;34m(model, tokenizer, prompt, hook_fns)\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;66;03m# ===== END CHANGE 7 =====\u001B[39;00m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad(): \n\u001B[0;32m---> 47\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\u001B[38;5;241m.\u001B[39mlogits[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1056\u001B[0m \u001B[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1060\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1062\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1063\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1064\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1066\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1067\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1070\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1071\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1072\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1073\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1074\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1076\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1077\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1079\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    910\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    911\u001B[0m         block\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    912\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m         output_attentions,\n\u001B[1;32m    920\u001B[0m     )\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 922\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    933\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    402\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    403\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[0;32m--> 404\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[1;32m    413\u001B[0m outputs \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:293\u001B[0m, in \u001B[0;36mGPT2Attention.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m encoder_attention_mask\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 293\u001B[0m     query_states, key_states, value_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_size, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    295\u001B[0m shape_q \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m*\u001B[39mquery_states\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[1;32m    296\u001B[0m shape_kv \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m*\u001B[39mkey_states\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/transformers/pytorch_utils.py:118\u001B[0m, in \u001B[0;36mConv1D.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    117\u001B[0m     size_out \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnf,)\n\u001B[0;32m--> 118\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddmm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(size_out)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    models_to_test = {\n",
    "    'gpt2': 'gpt2',  # Uncommented\n",
    "    'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "    'gptj6b': 'EleutherAI/gpt-j-6B',\n",
    "    'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "    'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "    'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "    #'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    main_output_dir = \"Results_6_WithChanges\"\n",
    "    os.makedirs(main_output_dir, exist_ok=True)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    for model_short_name, model_hf_name in models_to_test.items():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_hf_name)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- Could not load model {model_hf_name}. Skipping. Error: {e} ---\\n\")\n",
    "            continue\n",
    "\n",
    "        model_results_dir = os.path.join(main_output_dir, model_short_name)\n",
    "        os.makedirs(model_results_dir, exist_ok=True)\n",
    "\n",
    "        for task_name, task_data in datasets.items():\n",
    "            for patch_type in ['noising']:\n",
    "                result_key = f\"{task_name}_{patch_type}\"\n",
    "                df = run_exploratory_sweep(model, tokenizer, model_config, task_data, patch_type)\n",
    "\n",
    "                csv_path = os.path.join(model_results_dir, f\"{result_key}_results.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved data to: {csv_path}\")\n",
    "\n",
    "                plot_path_attn = os.path.join(model_results_dir, f\"{result_key}_attn_heads.pdf\")\n",
    "                plot_path_mlp = os.path.join(model_results_dir, f\"{result_key}_mlp_layers.pdf\")\n",
    "                title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                plot_results(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "                plot_results(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "        print(f\"--- Finished with {model_short_name}. Clearing memory. ---\")\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
