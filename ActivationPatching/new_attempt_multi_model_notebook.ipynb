{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Callable, List"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_model_config(model: nn.Module, model_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a standardized configuration dictionary for various model architectures.\n",
    "    This provides the correct hook names needed for our specific patching experiment.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"\n",
    "        })\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\",\n",
    "            \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"\n",
    "        })\n",
    "    elif 'llama' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\",\n",
    "            \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"\n",
    "        })\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model architecture for '{model_name}' not recognized. Please add its configuration.\")\n",
    "\n",
    "    return config\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Loads a pretrained Hugging Face model and tokenizer, handling various architectures.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    # --- ADD HUGGING FACE TOKEN HERE FOR GATED MODELS LIKE LLAMA ---\n",
    "    # Replace \"YOUR_HF_TOKEN_HERE\" with your actual token.\n",
    "    # It can be a read-only token for security.\n",
    "    HUGGING_FACE_TOKEN = \"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\"\n",
    "\n",
    "    model_dtype = torch.float16 if any(k in model_name.lower() for k in ['6b', '13b', '20b', '70b']) else torch.float32\n",
    "\n",
    "    if 'llama' in model_name.lower():\n",
    "        if HUGGING_FACE_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "            print(\"Warning: Llama model selected, but no Hugging Face token provided. This may fail.\")\n",
    "            access_token = None\n",
    "        else:\n",
    "            access_token = HUGGING_FACE_TOKEN\n",
    "\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True, token=access_token).to(device)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "    model_config = get_model_config(model, model_name)\n",
    "\n",
    "    return model, tokenizer, model_config"
   ],
   "id": "de925cccebe491d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Defines the clean and corrupted prompts and answers for each task.\n",
    "    This uses the single-prompt (\"toy data\") setup as specified.\n",
    "    \"\"\"\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs No Rule)\",\n",
    "            \"clean_prompt\": \"ABCD:ABCE::JKLM:JKL\",\n",
    "            \"clean_correct_answer\": \"N\",\n",
    "            \"clean_incorrect_answer\": \"M\",\n",
    "            \"corrupted_prompt\": \"ABCD:ABCD::JKLM:JKL\",\n",
    "            \"corrupted_correct_answer\": \"M\",\n",
    "            \"corrupted_incorrect_answer\": \"N\",\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"clean_prompt\": \"January:February::Wednesday:\",\n",
    "            \"clean_correct_answer\": \"Thursday\",\n",
    "            \"clean_incorrect_answer\": \"Wednesday\",\n",
    "            \"corrupted_prompt\": \"January:January::Wednesday:\",\n",
    "            \"corrupted_correct_answer\": \"Wednesday\",\n",
    "            \"corrupted_incorrect_answer\": \"Thursday\",\n",
    "        }\n",
    "    }\n",
    "    return datasets"
   ],
   "id": "6ca8a232f4254d07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "activation_cache: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "def caching_hook_factory(hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        activation_cache[hook_name] = tensor_to_cache.detach()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in activation_cache: raise ValueError(f\"Activation for {hook_name} not found!\")\n",
    "        cached_activation = activation_cache[hook_name]\n",
    "        patched_output = output.clone()\n",
    "        min_seq_len = min(patched_output.shape[-2], cached_activation.shape[-2])\n",
    "        if head_index is not None:\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            if patched_output.ndim == 3: patched_output[:, :min_seq_len, start:end] = cached_activation[:, :min_seq_len, start:end]\n",
    "            elif patched_output.ndim == 2: patched_output[:min_seq_len, start:end] = cached_activation[:min_seq_len, start:end]\n",
    "        else:\n",
    "            if patched_output.ndim == 3: patched_output[:, :min_seq_len, :] = cached_activation[:, :min_seq_len, :]\n",
    "            elif patched_output.ndim == 2: patched_output[:min_seq_len, :] = cached_activation[:min_seq_len, :]\n",
    "        return patched_output\n",
    "    return hook\n",
    "\n",
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'): model = getattr(model, part)\n",
    "    return model\n",
    "\n",
    "def run_with_hooks(model: nn.Module, tokenizer: GPT2Tokenizer, prompt: str, hook_fns: Dict[str, Callable]) -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module = get_module_by_name(model, name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(model.device)\n",
    "        with torch.no_grad(): outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles: handle.remove()\n",
    "\n",
    "def calculate_logit_diff(logits: torch.Tensor, tokenizer: GPT2Tokenizer, correct_answer: str, incorrect_answer: str) -> float:\n",
    "    try:\n",
    "        correct_id = tokenizer.encode(correct_answer, add_prefix_space=False)[0]\n",
    "        incorrect_id = tokenizer.encode(incorrect_answer, add_prefix_space=False)[0]\n",
    "        return (logits[correct_id] - logits[incorrect_id]).item()\n",
    "    except IndexError: return 0.0"
   ],
   "id": "bb5ed801d717061f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def perform_patching_experiment(model: nn.Module, tokenizer: GPT2Tokenizer, model_config: Dict, source_prompt: str, dest_prompt: str, dest_correct_answer: str, dest_incorrect_answer: str, layer: int, component_type: str, head_index: int = None) -> float:\n",
    "    global activation_cache\n",
    "    activation_cache = {}\n",
    "    hook_template = model_config['mlp_hook_name_template'] if component_type == 'mlp' else model_config['attn_hook_name_template']\n",
    "    hook_name = hook_template.format(layer)\n",
    "    run_with_hooks(model, tokenizer, source_prompt, {hook_name: caching_hook_factory(hook_name)})\n",
    "    d_head = model_config[\"d_model\"] // model_config[\"n_heads\"] if component_type == 'attn_head' else None\n",
    "    patching_hooks = {hook_name: patching_hook_factory(hook_name, head_index, d_head)}\n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(model: nn.Module, tokenizer: GPT2Tokenizer, model_config: Dict, task_data: Dict, patch_type: str) -> pd.DataFrame:\n",
    "    n_layers, n_heads = model_config[\"n_layers\"], model_config[\"n_heads\"]\n",
    "    if patch_type == 'noising':\n",
    "        source_prompt, dest_prompt = task_data['corrupted_prompt'], task_data['clean_prompt']\n",
    "        dest_correct, dest_incorrect = task_data['clean_correct_answer'], task_data['clean_incorrect_answer']\n",
    "    else: # denoising\n",
    "        source_prompt, dest_prompt = task_data['clean_prompt'], task_data['corrupted_prompt']\n",
    "        dest_correct, dest_incorrect = task_data['corrupted_correct_answer'], task_data['corrupted_incorrect_answer']\n",
    "\n",
    "    clean_logits = run_with_hooks(model, tokenizer, task_data['clean_prompt'], {})\n",
    "    clean_baseline = calculate_logit_diff(clean_logits, tokenizer, task_data['clean_correct_answer'], task_data['clean_incorrect_answer'])\n",
    "    corrupted_logits = run_with_hooks(model, tokenizer, task_data['corrupted_prompt'], {})\n",
    "    corrupted_baseline = calculate_logit_diff(corrupted_logits, tokenizer, task_data['corrupted_correct_answer'], task_data['corrupted_incorrect_answer'])\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}'\")\n",
    "\n",
    "    results = []\n",
    "    for component_type, head_range in [('attn_head', range(n_heads)), ('mlp', [-1])]:\n",
    "        print(f\"  - Patching {component_type}s...\")\n",
    "        for layer in range(n_layers):\n",
    "            for head_index in head_range:\n",
    "                patched_logit_diff = perform_patching_experiment(model, tokenizer, model_config, source_prompt, dest_prompt, dest_correct, dest_incorrect, layer, component_type, head_index if component_type == 'attn_head' else None)\n",
    "                if patch_type == 'noising':\n",
    "                    effect = patched_logit_diff - clean_baseline\n",
    "                else:\n",
    "                    effect = patched_logit_diff - corrupted_baseline\n",
    "                results.append({'layer': layer, 'head': head_index, 'type': component_type, 'effect': effect})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "b32dcfdfe06c5e17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_results(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    if component_type == 'attn_head':\n",
    "        if df[df['type'] == 'attn_head'].empty: return\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='head', columns='layer', values='effect')\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        max_abs_val = pivot_df.abs().max().max() if not pivot_df.empty else 1.0\n",
    "        im = ax.imshow(pivot_df, cmap='coolwarm', vmin=-max_abs_val, vmax=max_abs_val, aspect='auto')\n",
    "        cbar = ax.figure.colorbar(im, ax=ax); cbar.ax.set_ylabel(\"Effect on Logit Difference\", rotation=-90, va=\"bottom\")\n",
    "        ax.set_xticks(np.arange(pivot_df.shape[1])); ax.set_yticks(np.arange(pivot_df.shape[0]))\n",
    "        ax.set_xticklabels(pivot_df.columns); ax.set_yticklabels(pivot_df.index)\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Head Index\")\n",
    "    elif component_type == 'mlp':\n",
    "        if df[df['type'] == 'mlp'].empty: return\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(mlp_df['layer'], mlp_df['effect'], color='skyblue')\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Average Effect on Logit Difference\")\n",
    "        ax.grid(axis='y', linestyle='--'); ax.set_xticks(mlp_df['layer'])\n",
    "    else: raise ValueError(\"Invalid component type\")\n",
    "    ax.set_title(title); fig.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf'); print(f\"Saved plot to: {output_path}\")\n",
    "    plt.show(); plt.close(fig)"
   ],
   "id": "3467394fb471dc86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the activation patching experiment across multiple models.\n",
    "    \"\"\"\n",
    "    # This model dictionary is taken directly from the user's script\n",
    "    models_to_test = {\n",
    "        #'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        #'gpt2': 'gpt2', # Added gpt2 for a quick baseline\n",
    "        'gptj6b': 'EleutherAI/gpt-j-6B',\n",
    "        'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "        'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "        'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "        'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    main_output_dir = \"Results\"\n",
    "    os.makedirs(main_output_dir, exist_ok=True)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    for model_short_name, model_hf_name in models_to_test.items():\n",
    "        try:\n",
    "            model, tokenizer, model_config = setup_model_and_tokenizer(model_hf_name)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- Could not load model {model_hf_name}. Skipping. Error: {e} ---\\n\")\n",
    "            continue\n",
    "\n",
    "        # Create model-specific subdirectory inside the main \"Results\" folder\n",
    "        model_results_dir = os.path.join(main_output_dir, model_short_name)\n",
    "        os.makedirs(model_results_dir, exist_ok=True)\n",
    "\n",
    "        for task_name, task_data in datasets.items():\n",
    "            for patch_type in ['noising', 'denoising']:\n",
    "                result_key = f\"{task_name}_{patch_type}\"\n",
    "                df = run_exploratory_sweep(model, tokenizer, model_config, task_data, patch_type)\n",
    "\n",
    "                # Save CSV and Plots inside the model-specific subfolder\n",
    "                csv_path = os.path.join(model_results_dir, f\"{result_key}_results.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved data to: {csv_path}\")\n",
    "\n",
    "                plot_path_attn = os.path.join(model_results_dir, f\"{result_key}_attn_heads.pdf\")\n",
    "                plot_path_mlp = os.path.join(model_results_dir, f\"{result_key}_mlp_layers.pdf\")\n",
    "                title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                plot_results(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "                plot_results(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "        print(f\"--- Finished with {model_short_name}. Clearing memory. ---\")\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # --- NEW: Delete ONLY this model's cache ---\n",
    "        from transformers import file_utils\n",
    "        import shutil\n",
    "        import re\n",
    "\n",
    "        # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "        model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "        cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "\n",
    "        # 2. Delete only this model's folder\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Deleting model cache: {cache_path}\")\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)"
   ],
   "id": "26bcdfdc6df219ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "c4650f51bc272a4a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
