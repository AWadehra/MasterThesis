{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Callable, List\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658c1433-df4e-4e5f-bdfd-1f726046072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 1: ADD DETERMINISM FOUNDATION =====\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "# ===== END CHANGE 1 ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de925cccebe491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(model: nn.Module, model_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a standardized configuration dictionary for various model architectures.\n",
    "    This provides the correct hook names needed for our specific patching experiment.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"\n",
    "        })\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head,\n",
    "            \"d_model\": model.config.n_embd,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\",\n",
    "            \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"\n",
    "        })\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\",\n",
    "            \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"\n",
    "        })\n",
    "    elif 'llama' in model_name_lower:\n",
    "        config.update({\n",
    "            \"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads,\n",
    "            \"d_model\": model.config.hidden_size,\n",
    "            \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\",\n",
    "            \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"\n",
    "        })\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model architecture for '{model_name}' not recognized. Please add its configuration.\")\n",
    "\n",
    "    return config\n",
    "\n",
    "def setup_model_and_tokenizer(model_name: str, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Loads a pretrained Hugging Face model and tokenizer, handling various architectures.\n",
    "    \"\"\"\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    # --- ADD HUGGING FACE TOKEN HERE FOR GATED MODELS LIKE LLAMA ---\n",
    "    # Replace \"YOUR_HF_TOKEN_HERE\" with your actual token.\n",
    "    # It can be a read-only token for security.\n",
    "    HUGGING_FACE_TOKEN = \"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\"\n",
    "\n",
    "    model_dtype = torch.float16 if any(k in model_name.lower() for k in ['6b', '13b', '20b', '70b']) else torch.float32\n",
    "\n",
    "    if 'llama' in model_name.lower():\n",
    "        if HUGGING_FACE_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "            print(\"Warning: Llama model selected, but no Hugging Face token provided. This may fail.\")\n",
    "            access_token = None\n",
    "        else:\n",
    "            access_token = HUGGING_FACE_TOKEN\n",
    "\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True, token=access_token).to(device)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # ===== CHANGE 6: LAYERNORM STABILIZATION =====\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.LayerNorm):\n",
    "            module.eps = 1e-3  # Increased from 1e-5/1e-6\n",
    "    # ===== END CHANGE 6 =====\n",
    "    \n",
    "    model_config = get_model_config(model, model_name)\n",
    "\n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8c5e770643139b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\n",
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt2' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neo-125m' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_heads,\n",
    "                      \"n_layers\":model.config.num_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neo.layers.{layer}.attention.dense' for layer in range(model.config.num_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neo.layers.{layer}' for layer in range(model.config.num_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower() or 'pythia' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neox.layers.{layer}.attention.dense' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neox.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                token=access_token\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower() or '8b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else: #half precision for bigger llama models\n",
    "                #This becomes only for the 13B model then. Okay then. What else?\n",
    "                model_dtype = torch.float16\n",
    "\n",
    "            # If transformers version is < 4.31 use LlamaLoaders\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            # model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "            # If transformers version is >= 4.31, use AutoLoaders\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, token=access_token).to(device)\n",
    "\n",
    "        # --- THIS IS THE FIX ---\n",
    "        if tokenizer.pad_token is None:\n",
    "            print(\"Llama tokenizer does not have a pad token. Setting pad_token = eos_token.\")\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.config.pad_token_id = model.config.eos_token_id\n",
    "        \n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\":model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config._name_or_path,\n",
    "                      \"attn_hook_names\":[f'model.layers.{layer}.self_attn.o_proj' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'model.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":True}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e81171-06f0-4c34-8a1f-ef91dbdff202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer. This version is corrected to\n",
    "    handle all model architectures and create the correct config dictionary.\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "    print(f\"--- Loading model and tokenizer for '{model_name}' ---\")\n",
    "\n",
    "    HUGGING_FACE_TOKEN = \"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\" # User's provided token\n",
    "    kwargs = {'low_cpu_mem_usage': True}\n",
    "    \n",
    "    # --- Model Loading Logic ---\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        print(\"Using float16 revision for gpt-j-6B to ensure PyTorch-only workflow.\")\n",
    "        kwargs['revision'] = 'float16'\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    elif 'llama' in model_name.lower():\n",
    "        access_token = HUGGING_FACE_TOKEN if HUGGING_FACE_TOKEN != \"YOUR_HF_TOKEN_HERE\" else None\n",
    "        if not access_token: print(\"Warning: Llama model selected, but no Hugging Face token provided.\")\n",
    "        kwargs['token'] = access_token\n",
    "        if '70b' in model_name.lower():\n",
    "            bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "            kwargs['quantization_config'] = bnb_config\n",
    "            kwargs['trust_remote_code'] = True\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "        else:\n",
    "            kwargs['torch_dtype'] = torch.float16 if any(k in model_name.lower() for k in ['13b']) else torch.float32\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16 if '20b' in model_name.lower() else torch.float32\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device)\n",
    "        \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # --- THIS IS THE FIX: Create the config dict with the correct keys ---\n",
    "    model_config = {}\n",
    "    model_name_lower = model_name.lower()\n",
    "    if 'gpt-j' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.fc_out\"})\n",
    "    elif 'gpt2' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.n_layer, \"n_heads\": model.config.n_head, \"d_model\": model.config.n_embd, \"attn_hook_name_template\": \"transformer.h.{}.attn.c_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neo' in model_name_lower and 'gpt-neox' not in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_layers, \"n_heads\": model.config.num_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"transformer.h.{}.attn.out_proj\", \"mlp_hook_name_template\": \"transformer.h.{}.mlp.c_proj\"})\n",
    "    elif 'gpt-neox' in model_name_lower or 'pythia' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"gpt_neox.layers.{}.attention.dense\", \"mlp_hook_name_template\": \"gpt_neox.layers.{}.mlp.dense_4h_to_h\"})\n",
    "    elif 'llama' in model_name_lower:\n",
    "        model_config.update({\"n_layers\": model.config.num_hidden_layers, \"n_heads\": model.config.num_attention_heads, \"d_model\": model.config.hidden_size, \"attn_hook_name_template\": \"model.layers.{}.self_attn.o_proj\", \"mlp_hook_name_template\": \"model.layers.{}.mlp.down_proj\"})\n",
    "    else: raise NotImplementedError(\"Model architecture not recognized.\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca8a232f4254d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_datasets() -> Dict[str, Dict]:\n",
    "    \"\"\"Defines the datasets for each task, loading and formatting from JSON files.\"\"\"\n",
    "    \n",
    "    # ===== CHANGE 2: SEED PROMPT SAMPLING =====\n",
    "    random.seed(SEED)\n",
    "    # ===== END CHANGE 2 =====\n",
    "    \n",
    "    # --- Analogy Task Dataset Generation ---\n",
    "    with open('../BaselineAccuracy/dataset_files/abstractive/succ_letterstring_basic.json', 'r') as f:\n",
    "        analogy_data = json.load(f)\n",
    "\n",
    "    analogy_prompts = []\n",
    "    # Take a random sample of 20 to ensure variety each time the script is run\n",
    "    for _ in range(20):\n",
    "        # Ensure the example and target prompts are different\n",
    "        # ===== CHANGE 2: USE CHOICE INSTEAD OF SAMPLE =====\n",
    "        while True:\n",
    "            example_pair, target_pair = random.sample(analogy_data, 2)\n",
    "            if example_pair['input'] != target_pair['input']:\n",
    "                break\n",
    "        # ===== END CHANGE 2 =====\n",
    "\n",
    "        # Clean up the strings from the JSON file and format them\n",
    "        # e.g., \"[e f g h]\" -> \"e:f:g:h\"\n",
    "        example_in = example_pair['input'].strip('[]').replace(' ', '')\n",
    "        example_out = example_pair['output'].strip('[]').replace(' ', '')\n",
    "        target_in_full = target_pair['input'].strip('[]').replace(' ', '')\n",
    "        target_out_full = target_pair['output'].strip('[]').replace(' ', '')\n",
    "\n",
    "        # Create the prefix and single-token answers\n",
    "        #target_in_parts = target_in_full.split(':')\n",
    "        target_prefix = target_out_full[:3]\n",
    "        correct_answer = target_out_full[-1]\n",
    "        incorrect_answer = target_in_full[-1]\n",
    "\n",
    "        analogy_prompts.append({\n",
    "            \"clean_prompt\": f\"{example_in}:{example_out}::{target_in_full}:{target_prefix}\",\n",
    "            \"clean_correct_answer\": correct_answer,\n",
    "            \"clean_incorrect_answer\": incorrect_answer,\n",
    "            \"corrupted_prompt\": f\"{example_in}:{example_in}::{target_in_full}:{target_prefix}\",\n",
    "            \"corrupted_correct_answer\": incorrect_answer,\n",
    "            \"corrupted_incorrect_answer\": correct_answer,\n",
    "        })\n",
    "\n",
    "    # --- Sequencing Task Dataset Generation ---\n",
    "    with open('../BaselineAccuracy/dataset_files/abstractive/next_item.json', 'r') as f:\n",
    "        sequencing_data = json.load(f)\n",
    "\n",
    "    sequencing_prompts = []\n",
    "    # Take a random sample of 20\n",
    "    for _ in range(20):\n",
    "        # ===== CHANGE 2: USE CHOICE INSTEAD OF SAMPLE =====\n",
    "        while True:\n",
    "            example_pair, target_pair = random.sample(sequencing_data, 2)\n",
    "            if example_pair['input'] != target_pair['input']:\n",
    "                break\n",
    "        # ===== END CHANGE 2 =====\n",
    "\n",
    "        # Add leading spaces to ensure consistent tokenization\n",
    "        example_in, example_out =  example_pair['input'], example_pair['output']\n",
    "        target_in, target_out = target_pair['input'], target_pair['output']\n",
    "\n",
    "        sequencing_prompts.append({\n",
    "            \"clean_prompt\": f\"{example_in}:{example_out}::{target_in}:\",\n",
    "            \"clean_correct_answer\": target_out,\n",
    "            \"clean_incorrect_answer\": target_in,\n",
    "            \"corrupted_prompt\": f\"{example_in}:{example_in}::{target_in}:\",\n",
    "            \"corrupted_correct_answer\": target_in,\n",
    "            \"corrupted_incorrect_answer\": target_out,\n",
    "        })\n",
    "\n",
    "    datasets = {\n",
    "        \"analogy\": {\n",
    "            \"description\": \"Letter-String Analogy Task ('+1' vs No Rule)\",\n",
    "            \"prompts\": analogy_prompts\n",
    "        },\n",
    "        \"sequencing\": {\n",
    "            \"description\": \"Next-Item Sequencing Task\",\n",
    "            \"prompts\": sequencing_prompts\n",
    "        }\n",
    "    }\n",
    "    #print(datasets)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5ed801d717061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CHANGE 4: LOCAL CACHE ISOLATION =====\n",
    "def caching_hook_factory(cache: dict, hook_name: str) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        tensor_to_cache = output[0] if isinstance(output, tuple) else output\n",
    "        cache[hook_name] = tensor_to_cache.detach().clone()\n",
    "    return hook\n",
    "\n",
    "def patching_hook_factory(cache: dict, hook_name: str, head_index: int = None, d_head: int = None) -> Callable:\n",
    "    def hook(module, input, output):\n",
    "        if hook_name not in cache:\n",
    "            raise ValueError(f\"Activation for {hook_name} not found!\")\n",
    "        cached_activation = cache[hook_name]\n",
    "        patched_output = output.clone()\n",
    "        min_seq_len = min(patched_output.shape[-2], cached_activation.shape[-2])\n",
    "        if head_index is not None:\n",
    "            start, end = head_index * d_head, (head_index + 1) * d_head\n",
    "            if patched_output.ndim == 3: \n",
    "                patched_output[:, :min_seq_len, start:end] = cached_activation[:, :min_seq_len, start:end]\n",
    "            elif patched_output.ndim == 2: \n",
    "                patched_output[:min_seq_len, start:end] = cached_activation[:min_seq_len, start:end]\n",
    "        else:\n",
    "            if patched_output.ndim == 3: \n",
    "                patched_output[:, :min_seq_len, :] = cached_activation[:, :min_seq_len, :]\n",
    "            elif patched_output.ndim == 2: \n",
    "                patched_output[:min_seq_len, :] = cached_activation[:min_seq_len, :]\n",
    "        return patched_output\n",
    "    return hook\n",
    "# ===== END CHANGE 4 =====\n",
    "\n",
    "def get_module_by_name(model: nn.Module, name: str) -> nn.Module:\n",
    "    for part in name.split('.'): model = getattr(model, part)\n",
    "    return model\n",
    "\n",
    "def run_with_hooks(model: nn.Module, tokenizer: AutoTokenizer, prompt: str, hook_fns: Dict[str, Callable]) -> torch.Tensor:\n",
    "    handles = []\n",
    "    try:\n",
    "        for name, hook_fn in hook_fns.items():\n",
    "            module = get_module_by_name(model, name)\n",
    "            handles.append(module.register_forward_hook(hook_fn))\n",
    "            \n",
    "        # ===== CHANGE 7: ATTENTION MASK ENFORCEMENT =====\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move ALL to device\n",
    "        # ===== END CHANGE 7 =====\n",
    "        \n",
    "        with torch.no_grad(): outputs = model(**inputs)\n",
    "        return outputs.logits[0, -1, :]\n",
    "    finally:\n",
    "        for handle in handles: handle.remove()\n",
    "\n",
    "def calculate_logit_diff(logits: torch.Tensor, tokenizer: AutoTokenizer, correct_answer: str, incorrect_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the logit difference.\n",
    "    FIX: Uses `add_special_tokens=False` to prevent the tokenizer from adding\n",
    "    a Beginning-Of-Sentence token, which would make the logit difference always zero.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ===== CHANGE 3: TOKENIZATION SAFETY =====\n",
    "        correct_tokens = tokenizer.encode(correct_answer, add_special_tokens=False)\n",
    "        incorrect_tokens = tokenizer.encode(incorrect_answer, add_special_tokens=False)\n",
    "        \n",
    "        if not correct_tokens or not incorrect_tokens:\n",
    "            return 0.0\n",
    "            \n",
    "        correct_id = correct_tokens[0]\n",
    "        incorrect_id = incorrect_tokens[0]\n",
    "        # ===== END CHANGE 3 =====\n",
    "\n",
    "        return (logits[correct_id] - logits[incorrect_id]).item()\n",
    "\n",
    "    except IndexError:\n",
    "        # This can happen if the tokenizer returns an empty list for a given string\n",
    "        print(f\"Warning: Tokenizer failed to encode '{correct_answer}' or '{incorrect_answer}'.\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in calculate_logit_diff: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32dcfdfe06c5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_patching_experiment(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, source_prompt: str, dest_prompt: str, dest_correct_answer: str, dest_incorrect_answer: str, layer: int, component_type: str, head_index: int = None) -> float:\n",
    "   # ===== CHANGE 4: LOCAL CACHE ISOLATION =====\n",
    "    local_cache = {}\n",
    "    # ===== END CHANGE 4 =====\n",
    "    hook_template = model_config['mlp_hook_name_template'] if component_type == 'mlp' else model_config['attn_hook_name_template']\n",
    "    hook_name = hook_template.format(layer)\n",
    "    \n",
    "    # ===== CHANGE 4: USE LOCAL CACHE =====\n",
    "    run_with_hooks(model, tokenizer, source_prompt, \n",
    "                  {hook_name: caching_hook_factory(local_cache, hook_name)})\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    d_head = model_config[\"d_model\"] // model_config[\"n_heads\"] if component_type == 'attn_head' else None\n",
    "    \n",
    "    # ===== CHANGE 4: USE LOCAL CACHE =====\n",
    "    patching_hooks = {hook_name: patching_hook_factory(local_cache, hook_name, head_index, d_head)}\n",
    "    # ===== END CHANGE 4 =====\n",
    "    \n",
    "    patched_logits = run_with_hooks(model, tokenizer, dest_prompt, patching_hooks)\n",
    "    return calculate_logit_diff(patched_logits, tokenizer, dest_correct_answer, dest_incorrect_answer)\n",
    "\n",
    "def run_exploratory_sweep(model: nn.Module, tokenizer: AutoTokenizer, model_config: Dict, task_data: Dict, patch_type: str) -> pd.DataFrame:\n",
    "    n_layers, n_heads = model_config[\"n_layers\"], model_config[\"n_heads\"]\n",
    "    prompt_dataset = task_data['prompts']\n",
    "\n",
    "    print(f\"\\nRunning {patch_type} sweep for '{task_data['description']}' over {len(prompt_dataset)} prompts...\")\n",
    "\n",
    "    # Pre-calculate all baseline scores for efficiency\n",
    "    clean_baselines, corrupted_baselines = [], []\n",
    "    for prompt_set in prompt_dataset:\n",
    "        clean_logits = run_with_hooks(model, tokenizer, prompt_set['clean_prompt'], {})\n",
    "        clean_baselines.append(calculate_logit_diff(clean_logits, tokenizer, prompt_set['clean_correct_answer'], prompt_set['clean_incorrect_answer']))\n",
    "        corrupted_logits = run_with_hooks(model, tokenizer, prompt_set['corrupted_prompt'], {})\n",
    "        corrupted_baselines.append(calculate_logit_diff(corrupted_logits, tokenizer, prompt_set['corrupted_correct_answer'], prompt_set['corrupted_incorrect_answer']))\n",
    "\n",
    "    results = []\n",
    "    for component_type, head_range in [('attn_head', range(n_heads)), ('mlp', [-1])]:\n",
    "        print(f\"  - Patching {component_type}s...\")\n",
    "        for layer in range(n_layers):\n",
    "            for head_index in head_range:\n",
    "                effects_for_this_component = []\n",
    "                # Inner loop to iterate over the dataset for each component\n",
    "                for i, prompt_set in enumerate(prompt_dataset):\n",
    "                    #global activation_cache\n",
    "                    #activation_cache = {}\n",
    "                    if patch_type == 'noising':\n",
    "                        source_prompt, dest_prompt = prompt_set['corrupted_prompt'], prompt_set['clean_prompt']\n",
    "                        dest_correct, dest_incorrect = prompt_set['clean_correct_answer'], prompt_set['clean_incorrect_answer']\n",
    "                        baseline_to_compare = clean_baselines[i]\n",
    "                    else: # denoising\n",
    "                        source_prompt, dest_prompt = prompt_set['clean_prompt'], prompt_set['corrupted_prompt']\n",
    "                        dest_correct, dest_incorrect = prompt_set['corrupted_correct_answer'], prompt_set['corrupted_incorrect_answer']\n",
    "                        baseline_to_compare = corrupted_baselines[i]\n",
    "\n",
    "                    # Perform the patching experiment for this single prompt\n",
    "                    patched_logit_diff = perform_patching_experiment(model, tokenizer, model_config, source_prompt, dest_prompt, dest_correct, dest_incorrect, layer, component_type, head_index if component_type == 'attn_head' else None)\n",
    "                    effect = patched_logit_diff - baseline_to_compare\n",
    "                    effects_for_this_component.append(effect)\n",
    "\n",
    "                # Calculate the average effect across all prompts for this one component\n",
    "                average_effect = np.mean(effects_for_this_component)\n",
    "                results.append({'layer': layer, 'head': head_index, 'type': component_type, 'effect': average_effect})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3467394fb471dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df: pd.DataFrame, title: str, component_type: str, output_path: str = None):\n",
    "    if component_type == 'attn_head':\n",
    "        if df[df['type'] == 'attn_head'].empty: return\n",
    "        pivot_df = df[df['type'] == 'attn_head'].pivot(index='head', columns='layer', values='effect')\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        max_abs_val = pivot_df.abs().max().max() if not pivot_df.empty else 1.0\n",
    "        im = ax.imshow(pivot_df, cmap='coolwarm', vmin=-max_abs_val, vmax=max_abs_val, aspect='auto')\n",
    "        cbar = ax.figure.colorbar(im, ax=ax); cbar.ax.set_ylabel(\"Effect on Logit Difference\", rotation=-90, va=\"bottom\")\n",
    "        ax.set_xticks(np.arange(pivot_df.shape[1])); ax.set_yticks(np.arange(pivot_df.shape[0]))\n",
    "        ax.set_xticklabels(pivot_df.columns); ax.set_yticklabels(pivot_df.index)\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Head Index\")\n",
    "    elif component_type == 'mlp':\n",
    "        if df[df['type'] == 'mlp'].empty: return\n",
    "        mlp_df = df[df['type'] == 'mlp'].sort_values('layer')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.bar(mlp_df['layer'], mlp_df['effect'], color='skyblue')\n",
    "        ax.set_xlabel(\"Layer\"); ax.set_ylabel(\"Average Effect on Logit Difference\")\n",
    "        ax.grid(axis='y', linestyle='--'); ax.set_xticks(mlp_df['layer'])\n",
    "    else: raise ValueError(\"Invalid component type\")\n",
    "    ax.set_title(title); fig.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf'); print(f\"Saved plot to: {output_path}\")\n",
    "    plt.show(); plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26bcdfdc6df219ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the activation patching experiment across multiple models.\n",
    "    \"\"\"\n",
    "    # This model dictionary is taken directly from the user's script\n",
    "    models_to_test = {\n",
    "        #'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        #'gpt2': 'gpt2', # Added gpt2 for a quick baseline\n",
    "        #'gptj6b': 'EleutherAI/gpt-j-6b',\n",
    "        #'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "        #'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "        'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "        #'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    main_output_dir = \"Results_9_Random_MultiRun\"\n",
    "    os.makedirs(main_output_dir, exist_ok=True)\n",
    "    datasets = get_task_datasets()\n",
    "\n",
    "    for model_short_name, model_hf_name in models_to_test.items():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_hf_name)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- Could not load model {model_hf_name}. Skipping. Error: {e} ---\\n\")\n",
    "            \n",
    "            from transformers import file_utils\n",
    "            import shutil\n",
    "            import re\n",
    "    \n",
    "            # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "            model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "            cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "    \n",
    "            # 2. Delete only this model's folder\n",
    "            if os.path.exists(cache_path):\n",
    "                print(f\"Deleting model cache: {cache_path}\")\n",
    "                shutil.rmtree(cache_path, ignore_errors=True)\n",
    "                \n",
    "            continue\n",
    "\n",
    "        # Create model-specific subdirectory inside the main \"Results\" folder\n",
    "        model_results_dir = os.path.join(main_output_dir, model_short_name)\n",
    "        os.makedirs(model_results_dir, exist_ok=True)\n",
    "\n",
    "        for task_name, task_data in datasets.items():\n",
    "            for patch_type in ['noising']:\n",
    "                result_key = f\"{task_name}_{patch_type}\"\n",
    "                df = run_exploratory_sweep(model, tokenizer, model_config, task_data, patch_type)\n",
    "\n",
    "                # Save CSV and Plots inside the model-specific subfolder\n",
    "                csv_path = os.path.join(model_results_dir, f\"{result_key}_results.csv\")\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"Saved data to: {csv_path}\")\n",
    "\n",
    "                plot_path_attn = os.path.join(model_results_dir, f\"{result_key}_attn_heads.pdf\")\n",
    "                plot_path_mlp = os.path.join(model_results_dir, f\"{result_key}_mlp_layers.pdf\")\n",
    "                title_attn = f\"Attention Heads Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                title_mlp = f\"MLP Layers Effect ({patch_type.capitalize()})\\n{model_short_name} - {datasets[task_name]['description']}\"\n",
    "                plot_results(df, title_attn, 'attn_head', output_path=plot_path_attn)\n",
    "                plot_results(df, title_mlp, 'mlp', output_path=plot_path_mlp)\n",
    "\n",
    "        print(f\"--- Finished with {model_short_name}. Clearing memory. ---\")\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # --- NEW: Delete ONLY this model's cache ---\n",
    "        from transformers import file_utils\n",
    "        import shutil\n",
    "        import re\n",
    "\n",
    "        # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "        model_cache_name = f\"models--{re.sub(r'/', '--', model_hf_name)}\"\n",
    "        cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "\n",
    "        # 2. Delete only this model's folder\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Deleting model cache: {cache_path}\")\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4650f51bc272a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model and tokenizer for 'EleutherAI/gpt-neox-20b' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ed6941ceb445f69aaf64821b591db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running noising sweep for 'Letter-String Analogy Task ('+1' vs No Rule)' over 20 prompts...\n",
      "  - Patching attn_heads...\n",
      "  - Patching mlps...\n",
      "Saved data to: Results_9_Random_MultiRun/gptneox20b/analogy_noising_results.csv\n",
      "Saved plot to: Results_9_Random_MultiRun/gptneox20b/analogy_noising_attn_heads.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12686561/ipykernel_2401224/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: Results_9_Random_MultiRun/gptneox20b/analogy_noising_mlp_layers.pdf\n",
      "\n",
      "Running noising sweep for 'Next-Item Sequencing Task' over 20 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/awadehra.12686561/ipykernel_2401224/1871200346.py:23: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show(); plt.close(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Patching attn_heads...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
