{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportion_confint"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt2' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neo-125m' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_heads,\n",
    "                      \"n_layers\":model.config.num_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neo.layers.{layer}.attention.dense' for layer in range(model.config.num_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neo.layers.{layer}' for layer in range(model.config.num_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower() or 'pythia' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neox.layers.{layer}.attention.dense' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neox.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                token=access_token\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower() or '8b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else: #half precision for bigger llama models\n",
    "                #This becomes only for the 13B model then. Okay then. What else?\n",
    "                model_dtype = torch.float16\n",
    "\n",
    "            # If transformers version is < 4.31 use LlamaLoaders\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            # model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "            # If transformers version is >= 4.31, use AutoLoaders\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, token=access_token).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\":model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config._name_or_path,\n",
    "                      \"attn_hook_names\":[f'model.layers.{layer}.self_attn.o_proj' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'model.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":True}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG"
   ],
   "id": "2a88b7b0700e7177"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def check_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)"
   ],
   "id": "49e33a175dd914db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=40):\n",
    "    \"\"\"\n",
    "    Generate a response from the model for the given prompt\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response"
   ],
   "id": "32feb35ef11227ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def outputLLM(sentence, model, model_config, tokenizer, max_new_tokens=16):\n",
    "    \"\"\"\n",
    "    Allows for intervention in natural text where we generate and intervene on several tokens in a row.\n",
    "\n",
    "    Parameters:\n",
    "    sentence: sentence to intervene on with the FV\n",
    "    edit_layer: layer at which to add the function vector\n",
    "    function_vector: vector to add to the model that triggers execution of a task\n",
    "    model: huggingface model\n",
    "    model_config: dict with model config parameters (n_layers, n_heads, etc.)\n",
    "    tokenizer: huggingface tokenizer\n",
    "    max_new_tokens: number of tokens to generate\n",
    "    num_interv_tokens: number of tokens to apply the intervention for (defaults to all subsequent generations)\n",
    "    do_sample: whether to sample from top p tokens (True) or have deterministic greedy decoding (False)\n",
    "\n",
    "    Returns:\n",
    "    clean_output: tokens of clean output\n",
    "    intervention_output: tokens of intervention output\n",
    "\n",
    "    \"\"\"\n",
    "    # Clean Run, No Intervention:\n",
    "    device = model.device\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    clean_output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return clean_output"
   ],
   "id": "720e56e6f601e580"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_problems(num_permuted=1):\n",
    "    \"\"\"\n",
    "    Load nogen problems without shuffled letters\n",
    "    \"\"\"\n",
    "    prob_path = f'./problems/nogen/all_prob_{num_permuted}_7_human.npz'\n",
    "    if not os.path.exists(prob_path):\n",
    "        raise FileNotFoundError(f\"Problem file not found at {prob_path}\")\n",
    "\n",
    "    all_prob = np.load(prob_path, allow_pickle=True)['all_prob'].item()\n",
    "\n",
    "    # Filter out problems with shuffled letters\n",
    "    filtered_probs = {}\n",
    "    for alph in all_prob.keys():\n",
    "        if all_prob[alph]['shuffled_letters'] is None:\n",
    "            filtered_probs[alph] = all_prob[alph]\n",
    "\n",
    "    return filtered_probs"
   ],
   "id": "a815a5f5b08acf1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, tokenizer, problems, promptstyle='hw'):\n",
    "    \"\"\"\n",
    "    Evaluate model on all problems and return accuracy results\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    prob_types = ['succ']\n",
    "\n",
    "    for alph in problems.keys():\n",
    "        shuffled_alphabet = list(problems[alph]['shuffled_alphabet'])\n",
    "        alph_string = ' '.join(shuffled_alphabet)\n",
    "\n",
    "        for prob_type in prob_types:\n",
    "            if prob_type not in problems[alph]:\n",
    "                continue\n",
    "\n",
    "            for prob_ind in range(len(problems[alph][prob_type]['prob'])):\n",
    "                prob = problems[alph][prob_type]['prob'][prob_ind]\n",
    "\n",
    "                # Generate prompt\n",
    "                prompt = ''\n",
    "                if promptstyle == 'hw':\n",
    "                    #prompt += 'Use this fictional alphabet: \\n\\n' \\\n",
    "                    #          + alph_string \\\n",
    "                    #          + \"\\n\\nLet's try to complete the pattern:\\n\\n\"\n",
    "\n",
    "                    prompt += \"Let's try to complete the pattern:\\n\\n\"\n",
    "\n",
    "                # Format problem\n",
    "                prompt += '[' + ' '.join(prob[0][0]) + '] [' + ' '.join(prob[0][1]) + ']\\n[' \\\n",
    "                          + ' '.join(prob[1][0]) + '] ['\n",
    "\n",
    "                # Get model response\n",
    "                response = generate_response(prompt, model, tokenizer)\n",
    "\n",
    "                # Process response\n",
    "                first_bracket = response.find(']')\n",
    "                if first_bracket != -1:\n",
    "                    response = response[:first_bracket]\n",
    "                given_answer = [a for a in list(response) if a not in ' []']\n",
    "\n",
    "                # Get correct answer\n",
    "                if prob_type == 'attn':\n",
    "                    correct_answer = ['a', 'a', 'a', 'a']\n",
    "                else:\n",
    "                    correct_answer = prob[1][1]\n",
    "\n",
    "                # Check correctness\n",
    "                if len(correct_answer) != len(given_answer):\n",
    "                    incorrect = 1\n",
    "                else:\n",
    "                    incorrect = sum([a!=b for a, b in zip(correct_answer, given_answer)])\n",
    "                correct = not incorrect\n",
    "\n",
    "                data.append({\n",
    "                    'alph': alph,\n",
    "                    'prob_type': prob_type,\n",
    "                    'prob_ind': prob_ind,\n",
    "                    'source_1': prob[0][0],\n",
    "                    'source_2': prob[0][1],\n",
    "                    'target_1': prob[1][0],\n",
    "                    'correct_answer': correct_answer,\n",
    "                    'given_answer': given_answer,\n",
    "                    'correct': correct\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ],
   "id": "6bc1e9c2a7b59246"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_accuracy(results_df, model_name):\n",
    "    \"\"\"\n",
    "    Calculate and print accuracy statistics\n",
    "    \"\"\"\n",
    "    # Overall accuracy\n",
    "    overall_acc = results_df['correct'].mean()\n",
    "    ci_low, ci_high = proportion_confint(sum(results_df['correct']), len(results_df))\n",
    "    print(f\"Overall accuracy: {overall_acc:.3f} ({ci_low:.3f}-{ci_high:.3f})\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Accuracy by problem type\n",
    "    print(\"\\nAccuracy by problem type:\")\n",
    "    for prob_type, group in results_df.groupby('prob_type'):\n",
    "        acc = group['correct'].mean()\n",
    "        ci_low, ci_high = proportion_confint(sum(group['correct']), len(group))\n",
    "        print(f\"{prob_type}: {acc:.3f} ({ci_low:.3f}-{ci_high:.3f})\")\n",
    "    \"\"\"\n",
    "\n",
    "    data = {'Accuracy': overall_acc, 'CI_low': ci_low, 'CI_high': ci_high, 'model_name' : model_name }\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    return df"
   ],
   "id": "b8e389c9cc7d2fe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_results(accuracy_stats, model_name, output_dir='results'):\n",
    "    \"\"\"\n",
    "    Save evaluation results to files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save raw results\n",
    "    accuracy_stats.to_csv(os.path.join(output_dir, f'{model_name}.csv'), index=False)\n",
    "\n",
    "    #print(f\"Results saved to {output_dir}\")"
   ],
   "id": "e743e69020124fcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    # Load model\n",
    "\n",
    "    models = {\n",
    "        'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        'gpt2':'gpt2'}\n",
    "\n",
    "    for model_name, model_technical_name in models.items():\n",
    "        #model_technical_name = 'gpt2'\n",
    "        torch.cuda.empty_cache()  # Clear cache before loading new model\n",
    "        model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_technical_name)\n",
    "\n",
    "        # Load problems (unpermuted alphabet)\n",
    "        problems = load_problems(num_permuted=1)\n",
    "\n",
    "        # Evaluate model\n",
    "        results_df = evaluate_model(model, tokenizer, problems)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy_stats = calculate_accuracy(results_df, model_name)\n",
    "\n",
    "        # Save results\n",
    "        save_results(accuracy_stats, model_name)\n",
    "\n",
    "        # Clean up\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()  # Clear cache after evaluation"
   ],
   "id": "d2c54696c94a9ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8a348d50116c7b7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
