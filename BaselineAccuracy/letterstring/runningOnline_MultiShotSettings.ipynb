{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:27.366450Z",
     "start_time": "2025-05-22T16:56:27.328840Z"
    }
   },
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:28.631845Z",
     "start_time": "2025-05-22T16:56:27.426100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt2' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neo-125m' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_heads,\n",
    "                      \"n_layers\":model.config.num_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neo.layers.{layer}.attention.dense' for layer in range(model.config.num_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neo.layers.{layer}' for layer in range(model.config.num_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower() or 'pythia' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neox.layers.{layer}.attention.dense' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neox.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                token=access_token\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower() or '8b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else: #half precision for bigger llama models\n",
    "                #This becomes only for the 13B model then. Okay then. What else?\n",
    "                model_dtype = torch.float16\n",
    "\n",
    "            # If transformers version is < 4.31 use LlamaLoaders\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            # model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "            # If transformers version is >= 4.31, use AutoLoaders\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, token=access_token).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\":model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config._name_or_path,\n",
    "                      \"attn_hook_names\":[f'model.layers.{layer}.self_attn.o_proj' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'model.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":True}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG"
   ],
   "id": "2fa69b48590d4ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 228\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:28.787584Z",
     "start_time": "2025-05-22T16:56:28.658673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=40):\n",
    "    \"\"\"\n",
    "    Generate a response from the model for the given prompt\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response"
   ],
   "id": "518beb51bbeaeb7c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:28.964449Z",
     "start_time": "2025-05-22T16:56:28.821451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def outputLLM(sentence, model, model_config, tokenizer, max_new_tokens=16):\n",
    "    \"\"\"\n",
    "    Allows for intervention in natural text where we generate and intervene on several tokens in a row.\n",
    "\n",
    "    Parameters:\n",
    "    sentence: sentence to intervene on with the FV\n",
    "    edit_layer: layer at which to add the function vector\n",
    "    function_vector: vector to add to the model that triggers execution of a task\n",
    "    model: huggingface model\n",
    "    model_config: dict with model config parameters (n_layers, n_heads, etc.)\n",
    "    tokenizer: huggingface tokenizer\n",
    "    max_new_tokens: number of tokens to generate\n",
    "    num_interv_tokens: number of tokens to apply the intervention for (defaults to all subsequent generations)\n",
    "    do_sample: whether to sample from top p tokens (True) or have deterministic greedy decoding (False)\n",
    "\n",
    "    Returns:\n",
    "    clean_output: tokens of clean output\n",
    "    intervention_output: tokens of intervention output\n",
    "\n",
    "    \"\"\"\n",
    "    # Clean Run, No Intervention:\n",
    "    device = model.device\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    clean_output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return clean_output"
   ],
   "id": "dc7a36c147689693",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:29.164799Z",
     "start_time": "2025-05-22T16:56:29.051257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_problems(num_permuted=1):\n",
    "    \"\"\"\n",
    "    Load nogen problems without shuffled letters\n",
    "    \"\"\"\n",
    "    prob_path = f'./problems/nogen/all_prob_{num_permuted}_7_human.npz'\n",
    "    if not os.path.exists(prob_path):\n",
    "        raise FileNotFoundError(f\"Problem file not found at {prob_path}\")\n",
    "\n",
    "    all_prob = np.load(prob_path, allow_pickle=True)['all_prob'].item()\n",
    "\n",
    "    # Filter out problems with shuffled letters\n",
    "    filtered_probs = {}\n",
    "    for alph in all_prob.keys():\n",
    "        if all_prob[alph]['shuffled_letters'] is None:\n",
    "            filtered_probs[alph] = all_prob[alph]\n",
    "\n",
    "    return filtered_probs"
   ],
   "id": "2c52cfedf43c03fe",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:29.326138Z",
     "start_time": "2025-05-22T16:56:29.189392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, tokenizer, problems,EDIT_LAYER, FV, model_config, num_shots, promptstyle='hw'):\n",
    "    \"\"\"\n",
    "    Evaluate model on all problems and return accuracy results\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    prob_types = ['succ']\n",
    "\n",
    "    for alph in problems.keys():\n",
    "        shuffled_alphabet = list(problems[alph]['shuffled_alphabet'])\n",
    "        alph_string = ' '.join(shuffled_alphabet)\n",
    "\n",
    "        for prob_type in prob_types:\n",
    "            if prob_type not in problems[alph]:\n",
    "                continue\n",
    "\n",
    "            all_problems = problems[alph][prob_type]['prob']\n",
    "\n",
    "            for prob_ind in range(len(all_problems)):\n",
    "                # Select random examples (excluding the target problem)\n",
    "                example_inds = [i for i in range(len(all_problems)) if i != prob_ind]\n",
    "                selected_inds = random.sample(example_inds, min(num_shots, len(example_inds)))\n",
    "                examples = [all_problems[i] for i in selected_inds]\n",
    "                prob = all_problems[prob_ind]\n",
    "\n",
    "                prompt = \"\"\n",
    "                # Build few-shot prompt\n",
    "                #prompt += \"Let's try to complete the pattern:\\n\\n\" if promptstyle == 'hw' else \"\"\n",
    "\n",
    "                # Add examples\n",
    "                for example in examples:\n",
    "                    prompt += '[' + ' '.join(example[0][0]) + '] [' + ' '.join(example[0][1]) + ']\\n['\n",
    "\n",
    "                # Add target question (without answer)\n",
    "                prompt += '[' + ' '.join(prob[0][0]) + '] [' + ' '.join(prob[0][1]) + ']\\n[' \\\n",
    "                          + ' '.join(prob[1][0]) + '] ['\n",
    "\n",
    "                print(prompt)\n",
    "                response = generate_response(prompt, model, tokenizer)\n",
    "\n",
    "                #co, io = fv_intervention_natural_text(prompt, EDIT_LAYER, FV, model, #model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "                #input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "                #input_length = input_ids.shape[1]\n",
    "\n",
    "                #response = tokenizer.decode(io[0][input_length:], skip_special_tokens=True)\n",
    "\n",
    "                # Process response\n",
    "                first_bracket = response.find(']')\n",
    "                if first_bracket != -1:\n",
    "                    response = response[:first_bracket]\n",
    "                given_answer = [a for a in list(response) if a not in ' []']\n",
    "\n",
    "                # Get correct answer\n",
    "                if prob_type == 'attn':\n",
    "                    correct_answer = ['a', 'a', 'a', 'a']\n",
    "                else:\n",
    "                    correct_answer = prob[1][1]\n",
    "\n",
    "                # Check correctness\n",
    "                if len(correct_answer) != len(given_answer):\n",
    "                    incorrect = 1\n",
    "                else:\n",
    "                    incorrect = sum([a!=b for a, b in zip(correct_answer, given_answer)])\n",
    "                correct = not incorrect\n",
    "\n",
    "                data.append({\n",
    "                    'alph': alph,\n",
    "                    'prob_type': prob_type,\n",
    "                    'prob_ind': prob_ind,\n",
    "                    'source_1': prob[0][0],\n",
    "                    'source_2': prob[0][1],\n",
    "                    'target_1': prob[1][0],\n",
    "                    'correct_answer': correct_answer,\n",
    "                    'given_answer': given_answer,\n",
    "                    'correct': correct\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ],
   "id": "89297c4bc85018bd",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:29.474866Z",
     "start_time": "2025-05-22T16:56:29.387916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(results_df, model_name):\n",
    "    \"\"\"\n",
    "    Calculate and print accuracy statistics\n",
    "    \"\"\"\n",
    "    # Overall accuracy\n",
    "    overall_acc = results_df['correct'].mean()\n",
    "    ci_low, ci_high = proportion_confint(sum(results_df['correct']), len(results_df))\n",
    "    print(f\"Overall accuracy: {overall_acc:.3f} ({ci_low:.3f}-{ci_high:.3f})\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Accuracy by problem type\n",
    "    print(\"\\nAccuracy by problem type:\")\n",
    "    for prob_type, group in results_df.groupby('prob_type'):\n",
    "        acc = group['correct'].mean()\n",
    "        ci_low, ci_high = proportion_confint(sum(group['correct']), len(group))\n",
    "        print(f\"{prob_type}: {acc:.3f} ({ci_low:.3f}-{ci_high:.3f})\")\n",
    "    \"\"\"\n",
    "\n",
    "    data = {'Accuracy': overall_acc, 'CI_low': ci_low, 'CI_high': ci_high, 'model_name' : model_name }\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    return df"
   ],
   "id": "144841555152b89f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:29.947577Z",
     "start_time": "2025-05-22T16:56:29.529330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results(accuracy_stats, model_name, sub_folder, output_dir='results'):\n",
    "    \"\"\"\n",
    "    Save evaluation results to files\n",
    "    \"\"\"\n",
    "    model_output_dir = os.path.join(output_dir, sub_folder)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save raw results\n",
    "    accuracy_stats.to_csv(os.path.join(model_output_dir, f'{model_name}.csv'), index=False)\n",
    "\n",
    "    #print(f\"Results saved to {output_dir}\")"
   ],
   "id": "4b6702cd7081b9e8",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:30.291352Z",
     "start_time": "2025-05-22T16:56:29.983761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Load model\n",
    "\n",
    "    models = {\n",
    "        'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        'gpt2': 'gpt2',\n",
    "\n",
    "        'gptj6b': 'EleutherAI/gpt-j-6B',\n",
    "        'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "        'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "        'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "        'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    edit_layers = {\n",
    "        'gptj6b': 9,\n",
    "        'gptneox20b': 15,\n",
    "        'llama27b': 11,\n",
    "        'llama213b': 14,\n",
    "        'llama270b': 26\n",
    "    }\n",
    "\n",
    "    for model_name, model_technical_name in models.items():\n",
    "        #model_technical_name = 'gpt2'\n",
    "        if model_name in edit_layers:\n",
    "            EDIT_LAYER = edit_layers[model_name]\n",
    "\n",
    "        torch.cuda.empty_cache()  # Clear cache before loading new model\n",
    "        model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_technical_name)\n",
    "\n",
    "        dataset = load_dataset('succ_letterstring_basic', seed=0)\n",
    "        mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer)\n",
    "\n",
    "        FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)\n",
    "\n",
    "\n",
    "        # Load problems (unpermuted alphabet)\n",
    "        problems = load_problems(num_permuted=1)\n",
    "\n",
    "        # Evaluate model\n",
    "        results_df = evaluate_model(model, tokenizer, problems, EDIT_LAYER, FV, model_config, num_shots=0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy_stats = calculate_accuracy(results_df, model_name)\n",
    "\n",
    "        # Save results\n",
    "        sub_folder = 'BASIC_TESTING'\n",
    "        save_results(accuracy_stats, model_name, sub_folder)\n",
    "\n",
    "        # Clean up\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()  # Clear cache after evaluation"
   ],
   "id": "f0d6ed7acbcb6d9b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T16:56:38.006323Z",
     "start_time": "2025-05-22T16:56:30.315950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "88391fb2b93b5c8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-neo-125m\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "gpt_neo.layers.0.attention.dense",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mLookupError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 32\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     29\u001B[39m model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_technical_name)\n\u001B[32m     31\u001B[39m dataset = load_dataset(\u001B[33m'\u001B[39m\u001B[33msucc_letterstring_basic\u001B[39m\u001B[33m'\u001B[39m, seed=\u001B[32m0\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m mean_activations = \u001B[43mget_mean_head_activations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     34\u001B[39m FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=\u001B[32m10\u001B[39m)\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# Load problems (unpermuted alphabet)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\UvA Code\\Master Thesis\\MasterThesis\\BaselineAccuracy\\letterstring\\..\\src\\utils\\extract_utils.py:91\u001B[39m, in \u001B[36mget_mean_head_activations\u001B[39m\u001B[34m(dataset, model, model_config, tokenizer, n_icl_examples, N_TRIALS, shuffle_labels, prefixes, separators, filter_set)\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     90\u001B[39m     prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, shuffle_labels=shuffle_labels)\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m activations_td,idx_map,idx_avg = \u001B[43mgather_attn_activations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     92\u001B[39m \u001B[43m                                                    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mattn_hook_names\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     93\u001B[39m \u001B[43m                                                    \u001B[49m\u001B[43mdummy_labels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdummy_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     94\u001B[39m \u001B[43m                                                    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     95\u001B[39m \u001B[43m                                                    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     96\u001B[39m \u001B[43m                                                    \u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     98\u001B[39m stack_initial = torch.vstack([split_activations_by_head(activations_td[layer].input, model_config) \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model_config[\u001B[33m'\u001B[39m\u001B[33mattn_hook_names\u001B[39m\u001B[33m'\u001B[39m]]).permute(\u001B[32m0\u001B[39m,\u001B[32m2\u001B[39m,\u001B[32m1\u001B[39m,\u001B[32m3\u001B[39m)\n\u001B[32m     99\u001B[39m stack_filtered = stack_initial[:,:,\u001B[38;5;28mlist\u001B[39m(idx_map.keys())]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\UvA Code\\Master Thesis\\MasterThesis\\BaselineAccuracy\\letterstring\\..\\src\\utils\\extract_utils.py:41\u001B[39m, in \u001B[36mgather_attn_activations\u001B[39m\u001B[34m(prompt_data, layers, dummy_labels, model, tokenizer, model_config)\u001B[39m\n\u001B[32m     38\u001B[39m idx_map, idx_avg = compute_duplicated_labels(token_labels, dummy_labels)\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# Access Activations \u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mTraceDict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_input\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_output\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m td:                \n\u001B[32m     42\u001B[39m     model(**inputs) \u001B[38;5;66;03m# batch_size x n_tokens x vocab_size, only want last token prediction\u001B[39;00m\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m td, idx_map, idx_avg\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\masterthesis\\Lib\\site-packages\\baukit\\nethook.py:164\u001B[39m, in \u001B[36mTraceDict.__init__\u001B[39m\u001B[34m(self, module, layers, retain_output, retain_input, clone, detach, retain_grad, edit_output, stop)\u001B[39m\n\u001B[32m    161\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m obj.get(layer, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    162\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m164\u001B[39m \u001B[38;5;28mself\u001B[39m[layer] = \u001B[43mTrace\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_output\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_output\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    168\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_input\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_input\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[43m    \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclone\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    170\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdetach\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdetach\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    171\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_grad\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    172\u001B[39m \u001B[43m    \u001B[49m\u001B[43medit_output\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptional_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43medit_output\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_last\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\masterthesis\\Lib\\site-packages\\baukit\\nethook.py:69\u001B[39m, in \u001B[36mTrace.__init__\u001B[39m\u001B[34m(self, module, layer, retain_output, retain_input, clone, detach, retain_grad, edit_output, stop)\u001B[39m\n\u001B[32m     67\u001B[39m \u001B[38;5;28mself\u001B[39m.layer = layer\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m layer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     module = \u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mretain_hook\u001B[39m(m, inputs, output):\n\u001B[32m     72\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m edit_output:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\VE\\masterthesis\\Lib\\site-packages\\baukit\\nethook.py:368\u001B[39m, in \u001B[36mget_module\u001B[39m\u001B[34m(model, name)\u001B[39m\n\u001B[32m    366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n == name:\n\u001B[32m    367\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m m\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(name)\n",
      "\u001B[31mLookupError\u001B[39m: gpt_neo.layers.0.attention.dense"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
