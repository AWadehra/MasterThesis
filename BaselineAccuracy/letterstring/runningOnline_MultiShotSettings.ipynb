{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:36.844269Z",
     "start_time": "2025-06-04T14:08:08.174372Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fa69b48590d4ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:37.881321Z",
     "start_time": "2025-06-04T14:09:37.793136Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_gpt_model_and_tokenizer(model_name:str, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a huggingface model and its tokenizer\n",
    "\n",
    "    Parameters:\n",
    "    model_name: huggingface name of the model to load (e.g. GPTJ: \"EleutherAI/gpt-j-6B\", or \"EleutherAI/gpt-j-6b\")\n",
    "    device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "    model: huggingface model\n",
    "    tokenizer: huggingface tokenizer\n",
    "    MODEL_CONFIG: config variables w/ standardized names\n",
    "\n",
    "    \"\"\"\n",
    "    assert model_name is not None\n",
    "\n",
    "    print(\"Loading: \", model_name)\n",
    "\n",
    "    if 'gpt-j' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt2' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.n_head,\n",
    "                      \"n_layers\":model.config.n_layer,\n",
    "                      \"resid_dim\":model.config.n_embd,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'transformer.h.{layer}.attn.out_proj' for layer in range(model.config.n_layer)],\n",
    "                      \"layer_hook_names\":[f'transformer.h.{layer}' for layer in range(model.config.n_layer)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neo-125m' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_heads,\n",
    "                      \"n_layers\":model.config.num_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neo.layers.{layer}.attention.dense' for layer in range(model.config.num_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neo.layers.{layer}' for layer in range(model.config.num_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'gpt-neox' in model_name.lower() or 'pythia' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\": model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config.name_or_path,\n",
    "                      \"attn_hook_names\":[f'gpt_neox.layers.{layer}.attention.dense' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'gpt_neox.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":False}\n",
    "\n",
    "    elif 'llama' in model_name.lower():\n",
    "        if '70b' in model_name.lower():\n",
    "            # use quantization. requires `bitsandbytes` library\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = LlamaTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = LlamaForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_config,\n",
    "                token=access_token\n",
    "            )\n",
    "        else:\n",
    "            if '7b' in model_name.lower() or '8b' in model_name.lower():\n",
    "                model_dtype = torch.float32\n",
    "            else: #half precision for bigger llama models\n",
    "                #This becomes only for the 13B model then. Okay then. What else?\n",
    "                model_dtype = torch.float16\n",
    "\n",
    "            # If transformers version is < 4.31 use LlamaLoaders\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "            # model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype).to(device)\n",
    "\n",
    "            # If transformers version is >= 4.31, use AutoLoaders\n",
    "            access_token = \"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\"\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=model_dtype, token=access_token).to(device)\n",
    "\n",
    "        MODEL_CONFIG={\"n_heads\":model.config.num_attention_heads,\n",
    "                      \"n_layers\":model.config.num_hidden_layers,\n",
    "                      \"resid_dim\":model.config.hidden_size,\n",
    "                      \"name_or_path\":model.config._name_or_path,\n",
    "                      \"attn_hook_names\":[f'model.layers.{layer}.self_attn.o_proj' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"layer_hook_names\":[f'model.layers.{layer}' for layer in range(model.config.num_hidden_layers)],\n",
    "                      \"prepend_bos\":True}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Still working to get this model available!\")\n",
    "\n",
    "\n",
    "    return model, tokenizer, MODEL_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "518beb51bbeaeb7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.060180Z",
     "start_time": "2025-06-04T14:09:38.041293Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=40):\n",
    "    \"\"\"\n",
    "    Generate a response from the model for the given prompt\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc7a36c147689693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.212245Z",
     "start_time": "2025-06-04T14:09:38.196103Z"
    }
   },
   "outputs": [],
   "source": [
    "def outputLLM(sentence, model, model_config, tokenizer, max_new_tokens=16):\n",
    "    \"\"\"\n",
    "    Allows for intervention in natural text where we generate and intervene on several tokens in a row.\n",
    "\n",
    "    Parameters:\n",
    "    sentence: sentence to intervene on with the FV\n",
    "    edit_layer: layer at which to add the function vector\n",
    "    function_vector: vector to add to the model that triggers execution of a task\n",
    "    model: huggingface model\n",
    "    model_config: dict with model config parameters (n_layers, n_heads, etc.)\n",
    "    tokenizer: huggingface tokenizer\n",
    "    max_new_tokens: number of tokens to generate\n",
    "    num_interv_tokens: number of tokens to apply the intervention for (defaults to all subsequent generations)\n",
    "    do_sample: whether to sample from top p tokens (True) or have deterministic greedy decoding (False)\n",
    "\n",
    "    Returns:\n",
    "    clean_output: tokens of clean output\n",
    "    intervention_output: tokens of intervention output\n",
    "\n",
    "    \"\"\"\n",
    "    # Clean Run, No Intervention:\n",
    "    device = model.device\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    clean_output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return clean_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c52cfedf43c03fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.412076Z",
     "start_time": "2025-06-04T14:09:38.353305Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_problems(num_permuted=1):\n",
    "    \"\"\"\n",
    "    Load nogen problems without shuffled letters\n",
    "    \"\"\"\n",
    "    prob_path = f'./problems/nogen/all_prob_{num_permuted}_7_human.npz'\n",
    "    if not os.path.exists(prob_path):\n",
    "        raise FileNotFoundError(f\"Problem file not found at {prob_path}\")\n",
    "\n",
    "    all_prob = np.load(prob_path, allow_pickle=True)['all_prob'].item()\n",
    "\n",
    "    # Filter out problems with shuffled letters\n",
    "    filtered_probs = {}\n",
    "    for alph in all_prob.keys():\n",
    "        if all_prob[alph]['shuffled_letters'] is None:\n",
    "            filtered_probs[alph] = all_prob[alph]\n",
    "\n",
    "    return filtered_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89297c4bc85018bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.574344Z",
     "start_time": "2025-06-04T14:09:38.498845Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, problems,EDIT_LAYER, FV, model_config, num_shots, promptstyle='hw'):\n",
    "    \"\"\"\n",
    "    Evaluate model on all problems and return accuracy results\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    prob_types = ['succ']\n",
    "\n",
    "    for alph in problems.keys():\n",
    "        shuffled_alphabet = list(problems[alph]['shuffled_alphabet'])\n",
    "        alph_string = ' '.join(shuffled_alphabet)\n",
    "\n",
    "        for prob_type in prob_types:\n",
    "            if prob_type not in problems[alph]:\n",
    "                continue\n",
    "\n",
    "            all_problems = problems[alph][prob_type]['prob']\n",
    "\n",
    "            for prob_ind in range(len(all_problems)):\n",
    "                # Select random examples (excluding the target problem)\n",
    "                example_inds = [i for i in range(len(all_problems)) if i != prob_ind]\n",
    "                selected_inds = random.sample(example_inds, min(num_shots, len(example_inds)))\n",
    "                examples = [all_problems[i] for i in selected_inds]\n",
    "                prob = all_problems[prob_ind]\n",
    "\n",
    "                prompt = \"\"\n",
    "                # Build few-shot prompt\n",
    "                #prompt += \"Let's try to complete the pattern:\\n\\n\" if promptstyle == 'hw' else \"\"\n",
    "\n",
    "                # Add examples\n",
    "                for example in examples:\n",
    "                    prompt += '[' + ' '.join(example[0][0]) + '] [' + ' '.join(example[0][1]) + ']\\n'\n",
    "\n",
    "                # Add target question (without answer)\n",
    "                prompt += '[' + ' '.join(prob[0][0]) + '] [' + ' '.join(prob[0][1]) + ']\\n[' \\\n",
    "                          + ' '.join(prob[1][0]) + '] [' +' '.join(prob[1][1][:-1])\n",
    "\n",
    "                #prompt += '[' + ' '.join(prob[1][0]) + '] [' +' '.join(prob[1][1][:-1])\n",
    "\n",
    "                #print(prompt)\n",
    "                #response = generate_response(prompt, model, tokenizer)\n",
    "\n",
    "                co, io = fv_intervention_natural_text(prompt, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "                input_length = input_ids.shape[1]\n",
    "\n",
    "                response = tokenizer.decode(io[0][input_length:], skip_special_tokens=True)\n",
    "\n",
    "                # Process response\n",
    "                first_bracket = response.find(']')\n",
    "                if first_bracket != -1:\n",
    "                    response = response[:first_bracket]\n",
    "                given_answer = [a for a in list(response) if a not in ' []']\n",
    "\n",
    "                # Get correct answer\n",
    "                if prob_type == 'attn':\n",
    "                    correct_answer = ['a', 'a', 'a', 'a']\n",
    "                else:\n",
    "                    correct_answer = prob[1][1][-1]\n",
    "\n",
    "                # Check correctness\n",
    "                if len(correct_answer) != len(given_answer):\n",
    "                    incorrect = 1\n",
    "                else:\n",
    "                    incorrect = sum([a!=b for a, b in zip(correct_answer, given_answer)])\n",
    "                correct = not incorrect\n",
    "\n",
    "                data.append({\n",
    "                    'alph': alph,\n",
    "                    'prob_type': prob_type,\n",
    "                    'prob_ind': prob_ind,\n",
    "                    'source_1': prob[0][0],\n",
    "                    'source_2': prob[0][1],\n",
    "                    'target_1': prob[1][0],\n",
    "                    'correct_answer': correct_answer,\n",
    "                    'given_answer': given_answer,\n",
    "                    'correct': correct\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "144841555152b89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.650810Z",
     "start_time": "2025-06-04T14:09:38.624816Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(results_df, model_name):\n",
    "    \"\"\"\n",
    "    Calculate and print accuracy statistics\n",
    "    \"\"\"\n",
    "    # Overall accuracy\n",
    "    overall_acc = results_df['correct'].mean()\n",
    "    ci_low, ci_high = proportion_confint(sum(results_df['correct']), len(results_df))\n",
    "    print(f\"Overall accuracy: {overall_acc:.3f} ({ci_low:.3f}-{ci_high:.3f})\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Accuracy by problem type\n",
    "    print(\"\\nAccuracy by problem type:\")\n",
    "    for prob_type, group in results_df.groupby('prob_type'):\n",
    "        acc = group['correct'].mean()\n",
    "        ci_low, ci_high = proportion_confint(sum(group['correct']), len(group))\n",
    "        print(f\"{prob_type}: {acc:.3f} ({ci_low:.3f}-{ci_high:.3f})\")\n",
    "    \"\"\"\n",
    "\n",
    "    data = {'Accuracy': overall_acc, 'CI_low': ci_low, 'CI_high': ci_high, 'model_name' : model_name }\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b6702cd7081b9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.765275Z",
     "start_time": "2025-06-04T14:09:38.731459Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_results(accuracy_stats, model_name, sub_folder, output_dir='results'):\n",
    "    \"\"\"\n",
    "    Save evaluation results to files\n",
    "    \"\"\"\n",
    "    model_output_dir = os.path.join(output_dir, sub_folder)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save raw results\n",
    "    accuracy_stats.to_csv(os.path.join(model_output_dir, f'{model_name}.csv'), index=False)\n",
    "\n",
    "    #print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0d6ed7acbcb6d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:38.878856Z",
     "start_time": "2025-06-04T14:09:38.844963Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load model\n",
    "\n",
    "    models = {\n",
    "        #'gptneo': 'EleutherAI/gpt-neo-125m',\n",
    "        #gpt2': 'gpt2',\n",
    "\n",
    "        #'gptj6b': 'EleutherAI/gpt-j-6B',\n",
    "        #'llama27b': 'meta-llama/Llama-2-7b-hf',\n",
    "        'llama213b': 'meta-llama/Llama-2-13b-hf',\n",
    "        #'gptneox20b': 'EleutherAI/gpt-neox-20b',\n",
    "        #'llama270b': 'meta-llama/Llama-2-70b-hf'\n",
    "    }\n",
    "\n",
    "    edit_layers = {\n",
    "        'gptj6b': 9,\n",
    "        'gptneox20b': 15,\n",
    "        'llama27b': 11,\n",
    "        'llama213b': 14,\n",
    "        'llama270b': 26\n",
    "    }\n",
    "\n",
    "    for model_name, model_technical_name in models.items():\n",
    "        #model_technical_name = 'gpt2'\n",
    "        if model_name in edit_layers:\n",
    "            EDIT_LAYER = edit_layers[model_name]\n",
    "\n",
    "        torch.cuda.empty_cache()  # Clear cache before loading new model\n",
    "        model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_technical_name)\n",
    "\n",
    "        dataset = load_dataset('next_item', seed=0)\n",
    "        mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer)\n",
    "\n",
    "        FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)\n",
    "\n",
    "\n",
    "        # Load problems (unpermuted alphabet)\n",
    "        problems = load_problems(num_permuted=1)\n",
    "\n",
    "        # Evaluate model\n",
    "        results_df = evaluate_model(model, tokenizer, problems, EDIT_LAYER, FV, model_config, num_shots=2)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy_stats = calculate_accuracy(results_df, model_name)\n",
    "\n",
    "        # Save results\n",
    "        sub_folder = 'Basic_NoPrompt_threeShot_lastToken_FVnextItem'\n",
    "        save_results(accuracy_stats, model_name, sub_folder)\n",
    "\n",
    "        # Clean up\n",
    "        del model, tokenizer, model_config\n",
    "        torch.cuda.empty_cache()  # Clear cache after evaluation\n",
    "\n",
    "        # --- NEW: Delete ONLY this model's cache ---\n",
    "        from transformers import file_utils\n",
    "        import shutil\n",
    "        import re\n",
    "\n",
    "        # 1. Get model's cache folder name (convert \"/\" to \"--\")\n",
    "        model_cache_name = f\"models--{re.sub(r'/', '--', model_technical_name)}\"\n",
    "        cache_path = os.path.join(file_utils.default_cache_path, model_cache_name)\n",
    "\n",
    "        # 2. Delete only this model's folder\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Deleting model cache: {cache_path}\")\n",
    "            shutil.rmtree(cache_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88391fb2b93b5c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:09:43.175641Z",
     "start_time": "2025-06-04T14:09:38.991259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  meta-llama/Llama-2-13b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5358e50b673043558e691decd602fe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awadehra/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/awadehra/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.929 (0.868-0.989)\n",
      "Deleting model cache: /home/awadehra/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
